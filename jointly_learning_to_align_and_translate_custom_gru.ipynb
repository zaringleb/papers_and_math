{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading https://files.pythonhosted.org/packages/35/b7/539d492854096df09229a7d0f373c5b4f26f896013f3e00c54172deefb9a/tensorflow_datasets-4.5.2-py3-none-any.whl (4.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.2MB 454kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting dataclasses\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Collecting typing_extensions\n",
      "  Downloading https://files.pythonhosted.org/packages/05/e4/baf0031e39cf545f0c9edd5b1a2ea12609b7fcba2d58e118b11753d68cf0/typing_extensions-4.0.1-py3-none-any.whl\n",
      "Collecting importlib_resources\n",
      "  Downloading https://files.pythonhosted.org/packages/24/1b/33e489669a94da3ef4562938cd306e8fa915e13939d7b8277cb5569cb405/importlib_resources-5.4.0-py3-none-any.whl\n",
      "Collecting zipp\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/df/d4a4974a3e3957fd1c1fa3082366d7fff6e428ddb55f074bf64876f8e8ad/zipp-3.6.0-py3-none-any.whl\n",
      "Collecting tensorflow_metadata\n",
      "  Downloading https://files.pythonhosted.org/packages/49/eb/c521f1d457f5e121cb467fa8ad65e7838ad5004d1b0966ebd245da782dfd/tensorflow_metadata-1.2.0-py3-none-any.whl (48kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 7.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading https://files.pythonhosted.org/packages/b6/c3/973676ceb86b60835bb3978c6db67a5dc06be6cfdbd14ef0f5a13e3fc9fd/dill-0.3.4-py2.py3-none-any.whl (86kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 9.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting promise\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/9c/fb5d48abfe5d791cd496e4242ebcf87a4bb2e0c3dcd6e0ae68c11426a528/promise-2.3.tar.gz\n",
      "Building wheels for collected packages: promise\n",
      "  Running setup.py bdist_wheel for promise ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/andysilv/.cache/pip/wheels/19/49/34/c3c1e78bcb954c49e5ec0d31784fe63d14d427f316b12fbde9\n",
      "Successfully built promise\n",
      "Installing collected packages: tensorflow-datasets, dataclasses, typing-extensions, importlib-resources, zipp, tensorflow-metadata, dill, promise\n",
      "Successfully installed dataclasses-0.8 dill-0.3.4 importlib-resources-5.4.0 promise-2.3 tensorflow-datasets-4.5.2 tensorflow-metadata-1.2.0 typing-extensions-4.0.1 zipp-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow_datasets dataclasses typing_extensions importlib_resources zipp tensorflow_metadata dill promise --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/ea/84c7247f5c96c5a1b619fe822fb44052081ccfbe487a49d4c888306adec7/nltk-3.6.7-py3-none-any.whl (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 901kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/90/37/b681d58e0867ea8958dcde0458e0632ce41b29b2772505788a37e25f26b4/regex-2022.1.18-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (670kB)\n",
      "\u001b[K    100% |████████████████████████████████| 675kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/63/f3/b7a1b8e40fd1bd049a34566eb353527bb9b8e9b98f8b6cf803bb64d8ce95/tqdm-4.62.3-py2.py3-none-any.whl (76kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 7.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/d5/0163eb0cfa0b673aa4fe1cd3ea9d8a81ea0f32e50807b0c295871e4aab2e/joblib-1.1.0-py2.py3-none-any.whl (306kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 5.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/48/58/c8aa6a8e62cc75f39fee1092c45d6b6ba684122697d7ce7d53f64f98a129/click-8.0.3-py3-none-any.whl (97kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 10.0MB/s a 0:00:01\n",
      "\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\" (from click->nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/a1/b153a0a4caf7a7e3f15c2cd56c7702e2cf3d89b1b359d1f1c5e59d68f4ce/importlib_metadata-4.8.3-py3-none-any.whl\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\" (from importlib-metadata; python_version < \"3.8\"->click->nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/e4/baf0031e39cf545f0c9edd5b1a2ea12609b7fcba2d58e118b11753d68cf0/typing_extensions-4.0.1-py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->click->nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/bd/df/d4a4974a3e3957fd1c1fa3082366d7fff6e428ddb55f074bf64876f8e8ad/zipp-3.6.0-py3-none-any.whl\n",
      "Installing collected packages: regex, tqdm, joblib, typing-extensions, zipp, importlib-metadata, click, nltk\n",
      "Successfully installed click-8.0.3 importlib-metadata-4.8.3 joblib-1.1.0 nltk-3.6.7 regex-2022.1.18 tqdm-4.62.3 typing-extensions-4.0.1 zipp-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datasets manually from https://www.statmt.org/wmt14/translation-task.html\n",
    "# This notebook targets ru-en translation task, so we use here appropriate datasets:\n",
    "# * 1M Yandex Corpus \n",
    "# * newstest2012...newstest2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dir = 'datasets'\n",
    "if not os.path.exists(datasets_dir):\n",
    "    os.makedirs(datasets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_TOKENIZER_PATH = 'en_tokenizer.json'\n",
    "RU_TOKENIZER_PATH = 'ru_tokenizer.json'\n",
    "NUM_WORDS = 30000\n",
    "NO_CACHED_TOKENIZER = False\n",
    "MAX_LENGTH = 50 + 2\n",
    "\n",
    "\n",
    "def init_tokenizer(tokenizer_path, texts):\n",
    "    if not os.path.exists(tokenizer_path) or NO_CACHED_TOKENIZER:\n",
    "        print('initializing tokenizer and storing it to', tokenizer_path)\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=NUM_WORDS,\n",
    "            filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
    "            lower=True, split=' ', char_level=False, oov_token='<UNK>',\n",
    "            document_count=0\n",
    "        )\n",
    "\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        with open(tokenizer_path, 'w') as f:\n",
    "            f.write(tokenizer.to_json())\n",
    "    else:\n",
    "        print('loading tokenizer from', tokenizer_path)\n",
    "        with open(tokenizer_path, 'r') as f:\n",
    "            tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_datasets():\n",
    "    config = tfds.translate.wmt.WmtConfig(\n",
    "        version=\"0.0.1\",\n",
    "        language_pair=(\"ru\", \"en\"),\n",
    "        subsets={\n",
    "            tfds.Split.TRAIN: [\"yandexcorpus\"],\n",
    "            tfds.Split.VALIDATION: [\"newstest2012\", 'newstest2013', 'newstest2014', 'newstest2015', 'newstest2016', 'newstest2017'],\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    data_dir = '/home/andysilv/yandexsdc/seminars/attention/tensorflow_datasets'\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    builder = tfds.builder(\"wmt_translate\", config=config, data_dir=data_dir)\n",
    "\n",
    "    download_config = tfds.download.DownloadConfig(manual_dir=datasets_dir, extract_dir=datasets_dir)\n",
    "    builder.download_and_prepare(download_config=download_config, download_dir=datasets_dir)\n",
    "    return builder.as_dataset(as_supervised=True)\n",
    "\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    return '<start> ' + s.decode('utf-8') + ' <end>'\n",
    "\n",
    "\n",
    "def build_tokenizers(datasets):\n",
    "    ru_texts, en_texts = zip(*[\n",
    "        (preprocess_sentence(ru), preprocess_sentence(en))\n",
    "        for ru, en in datasets['train'].as_numpy_iterator()])\n",
    "    en_tokenizer = init_tokenizer(EN_TOKENIZER_PATH, en_texts)\n",
    "    ru_tokenizer = init_tokenizer(RU_TOKENIZER_PATH, ru_texts)\n",
    "    \n",
    "    return ru_tokenizer, en_tokenizer, ru_texts, en_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Using custom data configuration ru-en\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer from en_tokenizer.json\n",
      "loading tokenizer from ru_tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "ru_tokenizer, en_tokenizer, ru_texts, en_texts = build_tokenizers(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = ru_tokenizer.texts_to_sequences(ru_texts)\n",
    "target_tensor = en_tokenizer.texts_to_sequences(en_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor = zip(*[(ru, en) for ru, en in zip(input_tensor, target_tensor)\n",
    "                                    if len(ru) <= MAX_LENGTH and len(en) <= MAX_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764070, 764070, 191018, 191018)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GruCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_unit):\n",
    "        super(GruCell, self).__init__()\n",
    "        ortho_initializer = tf.keras.initializers.Orthogonal()\n",
    "        normal_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.01)\n",
    "        self.W = tf.keras.layers.Dense(hidden_unit, kernel_initializer=normal_initializer)\n",
    "        self.Wr = tf.keras.layers.Dense(hidden_unit, kernel_initializer=normal_initializer)\n",
    "        self.Wz = tf.keras.layers.Dense(hidden_unit, kernel_initializer=normal_initializer)\n",
    "        self.U = tf.keras.layers.Dense(hidden_unit, kernel_initializer=ortho_initializer)\n",
    "        self.Ur = tf.keras.layers.Dense(hidden_unit, kernel_initializer=ortho_initializer)\n",
    "        self.Uz = tf.keras.layers.Dense(hidden_unit, kernel_initializer=ortho_initializer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        prev_h, x = inputs\n",
    "        \n",
    "        r = tf.sigmoid(self.Wr(x) + self.Ur(prev_h))\n",
    "        z = tf.sigmoid(self.Wz(x) + self.Uz(prev_h))\n",
    "        h_ = tf.tanh(self.W(x) + self.U(tf.multiply(prev_h, r)))\n",
    "        h = tf.multiply((1 - z), prev_h) + tf.multiply(z, h_)\n",
    "        return h\n",
    "        \n",
    "\n",
    "class GruWithContextCell(GruCell):\n",
    "    def __init__(self, hidden_unit):\n",
    "        super(GruWithContextCell, self).__init__(hidden_unit)\n",
    "        \n",
    "        normal_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.01)\n",
    "        self.C = tf.keras.layers.Dense(hidden_unit, kernel_initializer=normal_initializer)\n",
    "        self.Cz = tf.keras.layers.Dense(hidden_unit, kernel_initializer=normal_initializer)\n",
    "        self.Cr = tf.keras.layers.Dense(hidden_unit, kernel_initializer=normal_initializer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        prev_h, x, ctx = inputs\n",
    "        r = tf.sigmoid(self.Wr(x) + self.Ur(prev_h) + self.Cr(ctx))\n",
    "        z = tf.sigmoid(self.Wz(x) + self.Uz(prev_h) + self.Cz(ctx))\n",
    "        h_ = tf.tanh(self.W(x) + self.U(tf.multiply(prev_h, r) + self.C(ctx)))\n",
    "        h = tf.multiply((1 - z), prev_h) + tf.multiply(z, h_)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GruLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_unit, return_sequences, return_state, go_backwards=False):\n",
    "        super(GruLayer, self).__init__()\n",
    "        \n",
    "        self._gru_cell = GruCell(hidden_unit)\n",
    "        self._return_sequences = return_sequences\n",
    "        self._return_state = return_state\n",
    "        self._go_backwards = go_backwards\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        hidden, x = inputs\n",
    "        all_hidden = []\n",
    "        if not self._go_backwards:\n",
    "            for i in range(x.shape[1]):\n",
    "                hidden = self._gru_cell([hidden, x[:,i,:]])\n",
    "                all_hidden.append(hidden)\n",
    "        else:\n",
    "            for i in range(x.shape[1] - 1, -1, -1):\n",
    "                hidden = self._gru_cell([hidden, x[:,i,:]])\n",
    "                all_hidden.append(hidden)\n",
    "            all_hidden.reverse()\n",
    "        if self._return_sequences and self._return_state:\n",
    "            return all_hidden, hidden\n",
    "        elif self._return_sequences:\n",
    "            return all_hidden\n",
    "        elif self._return_state:\n",
    "            return hidden\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.forward_gru = GruLayer(\n",
    "            enc_units, return_sequences=True, \n",
    "            return_state=False)\n",
    "        self.backward_gru = GruLayer(\n",
    "            enc_units, return_sequences=True, \n",
    "            return_state=True)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        forward_output = self.forward_gru((hidden, x))\n",
    "        backward_output, state = self.backward_gru((hidden, x))\n",
    "        output = tf.concat([forward_output, backward_output], axis=-1)\n",
    "        output = tf.transpose(output, perm=[1, 0, 2])\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, maxout_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GruWithContextCell(dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.Ws = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W0 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.U0 = tf.keras.layers.Dense(2 * maxout_size)\n",
    "        self.V0 = tf.keras.layers.Dense(2 * maxout_size)\n",
    "        self.C0 = tf.keras.layers.Dense(2 * maxout_size)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.maxout = tfa.layers.Maxout(maxout_size)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        \n",
    "        state = self.gru([hidden, x, context_vector])\n",
    "        \n",
    "        t_ = self.U0(hidden) + self.V0(x) + self.C0(context_vector)\n",
    "        t = self.maxout(t_)\n",
    "        x = self.fc(t)\n",
    "        return x, state, attention_weights\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        # output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        # output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        # x = self.fc(output)\n",
    "        \n",
    "        # return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self, enc_hidden):\n",
    "        return tf.tanh(self.Ws(enc_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 80\n",
    "SORT_BATCH_SIZE = BATCH_SIZE * 20\n",
    "N_BATCH = BUFFER_SIZE // BATCH_SIZE\n",
    "WORDS_EMBEDDING_SIZE = 256\n",
    "HIDDEN_STATES = 1000\n",
    "INPUT_VOCAB_SIZE = NUM_WORDS + 1  # pad\n",
    "TARGET_VOCAB_SIZE = NUM_WORDS + 1  # pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_VOCAB_SIZE, WORDS_EMBEDDING_SIZE, HIDDEN_STATES)\n",
    "decoder = Decoder(TARGET_VOCAB_SIZE, WORDS_EMBEDDING_SIZE, HIDDEN_STATES, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # ignore paddings in loss, which have 0 index\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    nonzeros = np.count_nonzero(real)\n",
    "    assert nonzeros > 0\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_) * real.shape[0] / nonzeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints_custom'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got <tensorflow.python.util._pywrap_checkpoint_reader.CheckpointReader object at 0x7fee8c898d88>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-268bb90561e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2327\u001b[0m     \u001b[0morig_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m     if save_path is not None and gfile.IsDirectory(save_path) and (\n\u001b[0m\u001b[1;32m   2330\u001b[0m         (gfile.Exists(utils_impl.get_saved_model_pb_path(save_path)) or\n\u001b[1;32m   2331\u001b[0m          gfile.Exists(utils_impl.get_saved_model_pbtxt_path(save_path)))):\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mis_directory\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m   \"\"\"\n\u001b[0;32m--> 690\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mis_directory_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mis_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    702\u001b[0m   \"\"\"\n\u001b[1;32m    703\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_file_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mpath_to_bytes\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__fspath__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 87\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got <tensorflow.python.util._pywrap_checkpoint_reader.CheckpointReader object at 0x7fee8c898d88>"
     ]
    }
   ],
   "source": [
    "latest = tf.train.load_checkpoint(checkpoint_dir)\n",
    "# try:\n",
    "#     checkpoint.restore(latest)\n",
    "\n",
    "#     encoder = checkpoint.encoder\n",
    "#     decoder = checkpoint.decoder\n",
    "# except Excepation as e:\n",
    "#     print(e)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint.restore(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls training_checkpoints/ -lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "train_bleu = tf.keras.metrics.Mean('train_bleu', dtype=tf.float32)\n",
    "test_bleu = tf.keras.metrics.Mean('test_bleu', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape_custom_gru/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape_custom_gru/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))\n",
    "\n",
    "def translate_encoded(input_tensor):\n",
    "    num_sentences = len(input_tensor)\n",
    "    hidden = encoder.initialize_hidden_state(num_sentences)\n",
    "    \n",
    "    enc_output, enc_hidden = encoder(input_tensor, hidden)\n",
    "    dec_hidden = decoder.initialize_hidden_state(enc_hidden)\n",
    "    dec_input = np.array([[en_tokenizer.word_index['<start>']]])\n",
    "    \n",
    "    beam_width = 3\n",
    "    max_len = 50\n",
    "    initial_result = {\n",
    "        \"result\": [en_tokenizer.word_index['<start>']],\n",
    "        \"log_prob\": 0\n",
    "    }\n",
    "    \n",
    "    results_by_sentence = [[initial_result] for i in range(num_sentences)]\n",
    "    current_states = [dict(\n",
    "        dec_hidden=dec_hidden[idx],\n",
    "        dec_input=dec_input,\n",
    "        sentence_index=idx,\n",
    "        enc_output=enc_output[idx],\n",
    "        **initial_result\n",
    "    ) for idx in range(num_sentences)]\n",
    "    \n",
    "    for t in range(1, max_len):\n",
    "        new_current_states_by_sentence = [[] for sentence_index in range(num_sentences)]\n",
    "        \n",
    "        for current_states_chunk in chunks(current_states, BATCH_SIZE):            \n",
    "            dec_input = tf.concat([cs['dec_input'] for cs in current_states_chunk], -2)\n",
    "            enc_output = tf.concat([tf.expand_dims(cs['enc_output'], 0) for cs in current_states_chunk], 0)\n",
    "            dec_hidden = tf.concat([tf.expand_dims(cs['dec_hidden'], 0) for cs in current_states_chunk], 0)\n",
    "            predictions_batch, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            max_prob_inds_batch = np.argpartition(predictions_batch, -beam_width, axis=-1)[:, -beam_width:]\n",
    "            for idx, (current_state, max_prob_inds, predictions) in enumerate(zip(\n",
    "                    current_states_chunk, max_prob_inds_batch, predictions_batch)):\n",
    "                probs = np.exp(predictions) / np.sum(np.exp(predictions))\n",
    "                sentence_index = current_state['sentence_index']\n",
    "                for ind in max_prob_inds:\n",
    "                    res = {\n",
    "                        'log_prob': current_state['log_prob'] + np.log(probs[ind]),\n",
    "                        'result': current_state['result'] + [ind]\n",
    "                    }\n",
    "                    results_by_sentence[sentence_index].append(res)\n",
    "\n",
    "                    if ind == en_tokenizer.word_index['<end>']:\n",
    "                        continue\n",
    "                    new_current_states_by_sentence[sentence_index].append(dict(\n",
    "                        dec_hidden=dec_hidden[idx],\n",
    "                        dec_input=np.array([[ind]]),\n",
    "                        enc_output=current_states_chunk[idx]['enc_output'],\n",
    "                        sentence_index=sentence_index,\n",
    "                        **res\n",
    "                    ))\n",
    "        new_current_states = []\n",
    "        for new_states in new_current_states_by_sentence:\n",
    "            new_states.sort(key=lambda x: x['log_prob'])\n",
    "            new_current_states.extend(new_states[-beam_width:])\n",
    "        current_states = new_current_states\n",
    "    \n",
    "    for results in results_by_sentence:\n",
    "        for r in results:\n",
    "            r['normalized_log_prob'] = r['log_prob'] / len(r['result'])\n",
    "        results.sort(key=lambda k: -r['normalized_log_prob'])\n",
    "    return results_by_sentence\n",
    "\n",
    "\n",
    "def translate(texts):\n",
    "    if isinstance(texts, bytes):\n",
    "        texts = [texts]\n",
    "    texts = [preprocess_sentence(t) for t in texts]\n",
    "    input_tensor = ru_tokenizer.texts_to_sequences(texts)\n",
    "    max_len = max(len(t) for t in input_tensor)\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        input_tensor, \n",
    "        maxlen=max_len, padding='post', value=0)\n",
    "    return translate_encoded(input_tensor)\n",
    "    \n",
    "    \n",
    "def evaluate_bleu_on_batch(ru, en):\n",
    "    results_by_sentence = translate_encoded(ru)\n",
    "    chencherry = nltk.bleu_score.SmoothingFunction()\n",
    "    translations = []\n",
    "    for results in results_by_sentence:\n",
    "        translation = ''\n",
    "        for r in results:\n",
    "            if r['result'][-1] != en_tokenizer.word_index['<end>']:\n",
    "                continue\n",
    "            translation = r['result'][1:-1]\n",
    "            break\n",
    "        translations.append(translation)\n",
    "    return nltk.bleu_score.corpus_bleu([[np.trim_zeros(ref)[1:-1]] for ref in en], translations, smoothing_function=chencherry.method7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(encoder, decoder, optimizer, inp, targ):\n",
    "    encoder_initial_hidden = encoder.initialize_hidden_state(inp.shape[0])\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, encoder_initial_hidden)\n",
    "\n",
    "        dec_hidden = decoder.initialize_hidden_state(enc_hidden)\n",
    "  \n",
    "        dec_input = tf.expand_dims(targ[:, 0], 1)\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        loss = 0\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    batch_loss = loss / (targ.shape[1] - 1)\n",
    "    variables = encoder.variables + decoder.variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    train_loss(batch_loss)\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "def test_step(encoder, decoder, inp, targ):\n",
    "    encoder_initial_hidden = encoder.initialize_hidden_state(inp.shape[0])\n",
    "    enc_output, enc_hidden = encoder(inp, encoder_initial_hidden)\n",
    "    dec_hidden = decoder.initialize_hidden_state(enc_hidden)\n",
    "    dec_input = tf.expand_dims(targ[:, 0], 1)\n",
    "    loss = 0\n",
    "    for t in range(1, targ.shape[1]):\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "        loss += loss_function(targ[:, t], predictions)\n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    batch_loss = loss / (targ.shape[1] - 1)\n",
    "\n",
    "    test_loss(batch_loss)\n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensors(inp, tar):\n",
    "    max_length_inp = max(len(x) for x in inp)\n",
    "    max_length_tar = max(len(x) for x in tar)\n",
    "    inp = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        inp, maxlen=max_length_inp, padding='post', value=0)\n",
    "    tar = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tar, maxlen=max_length_tar, padding='post', value=0)\n",
    "    return inp, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 13700 Loss 2.5595 bleu 0.1885\n",
      "Epoch 1 Batch 13800 Loss 2.4354 bleu 0.1864\n",
      "Epoch 1 Batch 13900 Loss 2.2769 bleu 0.1687\n",
      "Epoch 1 Batch 14000 Loss 2.4552 bleu 0.1894\n",
      "Epoch 1 Batch 14100 Loss 2.2641 bleu 0.1364\n",
      "Epoch 1 Batch 14200 Loss 2.9326 bleu 0.1750\n",
      "Epoch 1 Batch 14300 Loss 2.2852 bleu 0.1588\n",
      "Epoch 1 Batch 14400 Loss 2.3218 bleu 0.1794\n",
      "Epoch 1 Batch 14500 Loss 2.3251 bleu 0.1595\n",
      "Epoch 1 Batch 14600 Loss 2.8363 bleu 0.1813\n",
      "Epoch 1 Batch 14700 Loss 2.3403 bleu 0.1702\n",
      "Epoch 1 Batch 14800 Loss 2.4451 bleu 0.1879\n",
      "Epoch 1 Batch 14900 Loss 2.6534 bleu 0.1428\n",
      "Epoch 1 Batch 15000 Loss 2.7176 bleu 0.1840\n",
      "Epoch 1 Batch 15100 Loss 2.0133 bleu 0.1704\n",
      "Epoch 1 Batch 15200 Loss 2.3532 bleu 0.1660\n",
      "Epoch 1 Batch 15300 Loss 2.9764 bleu 0.1930\n",
      "Epoch 1 Batch 15400 Loss 2.2706 bleu 0.1681\n",
      "Epoch 1 Batch 15500 Loss 2.3537 bleu 0.1680\n",
      "Epoch 1 Batch 15600 Loss 1.9770 bleu 0.1743\n",
      "Epoch 1 Batch 15700 Loss 1.8677 bleu 0.1723\n",
      "Epoch 1 Batch 15800 Loss 2.4400 bleu 0.1793\n",
      "Epoch 1 Batch 15900 Loss 1.8573 bleu 0.1715\n",
      "Epoch 1 Batch 16000 Loss 2.5423 bleu 0.1879\n",
      "Epoch 1 Batch 16100 Loss 1.9134 bleu 0.1631\n",
      "Epoch 1 Batch 16200 Loss 2.2457 bleu 0.1558\n",
      "Epoch 1 Batch 16300 Loss 2.3797 bleu 0.1757\n",
      "Epoch 1 Batch 16400 Loss 2.2077 bleu 0.1624\n",
      "Epoch 1 Batch 16500 Loss 2.0229 bleu 0.2007\n",
      "Epoch 1 Batch 16600 Loss 1.8673 bleu 0.1674\n",
      "Epoch 1 Batch 16700 Loss 2.4539 bleu 0.1611\n",
      "Epoch 1 Batch 16800 Loss 2.1295 bleu 0.1723\n",
      "Epoch 1 Batch 16900 Loss 2.1794 bleu 0.1805\n",
      "Epoch 1 Batch 17000 Loss 2.0761 bleu 0.1539\n",
      "Epoch 1 Batch 17100 Loss 2.2104 bleu 0.1983\n",
      "Epoch 1 Batch 17200 Loss 2.1272 bleu 0.1751\n",
      "Epoch 1 Batch 17300 Loss 2.5165 bleu 0.1783\n",
      "Epoch 1 Batch 17400 Loss 1.8939 bleu 0.1491\n",
      "Epoch 1 Batch 17500 Loss 2.4441 bleu 0.1907\n",
      "Epoch 1 Batch 17600 Loss 2.1010 bleu 0.1757\n",
      "Epoch 1 Batch 17700 Loss 2.3302 bleu 0.1771\n",
      "Epoch 1 Batch 17800 Loss 2.1283 bleu 0.1649\n",
      "Epoch 1 Batch 17900 Loss 3.0654 bleu 0.1761\n",
      "Epoch 1 Batch 18000 Loss 2.3939 bleu 0.1472\n",
      "Epoch 1 Batch 18100 Loss 2.7333 bleu 0.1650\n",
      "Epoch 1 Batch 18200 Loss 2.4305 bleu 0.1756\n",
      "Epoch 1 Batch 18300 Loss 2.7992 bleu 0.1649\n",
      "Epoch 1 Batch 18400 Loss 2.3681 bleu 0.1581\n",
      "Epoch 1 Batch 18500 Loss 3.1046 bleu 0.1594\n",
      "Epoch 1 Batch 18600 Loss 2.5960 bleu 0.1550\n",
      "Epoch 1 Batch 18700 Loss 2.2057 bleu 0.1697\n",
      "Epoch 1 Batch 18800 Loss 2.4521 bleu 0.1696\n",
      "Epoch 1 Batch 18900 Loss 2.1947 bleu 0.1741\n",
      "Epoch 1 Batch 19000 Loss 2.3688 bleu 0.1812\n",
      "Epoch 1 Batch 19100 Loss 2.8174 bleu 0.1662\n",
      "Epoch 1 Batch 19200 Loss 2.2927 bleu 0.1803\n",
      "Epoch 1 Batch 19300 Loss 2.1321 bleu 0.1805\n",
      "Epoch 1 Batch 19400 Loss 2.0159 bleu 0.1755\n",
      "Epoch 1 Batch 19500 Loss 2.1874 bleu 0.1731\n",
      "Epoch 1 Batch 19600 Loss 2.7766 bleu 0.1812\n",
      "Epoch 1 Batch 19700 Loss 2.4133 bleu 0.1557\n",
      "Epoch 1 Batch 19800 Loss 2.5173 bleu 0.1838\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "bleu_every_n_steps = 10\n",
    "log_metrics_every_n_steps = 100\n",
    "\n",
    "val_bleu_iter = iter(zip(chunks(input_tensor_val, BATCH_SIZE),\n",
    "                     chunks(target_tensor_val, BATCH_SIZE)))\n",
    "val_loss_iter = iter(zip(chunks(input_tensor_val, BATCH_SIZE),\n",
    "                     chunks(target_tensor_val, BATCH_SIZE)))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    train_dataset = zip(chunks(input_tensor_train, SORT_BATCH_SIZE),\n",
    "                        chunks(target_tensor_train, SORT_BATCH_SIZE))\n",
    "    val_dataset = zip(chunks(input_tensor_val, BATCH_SIZE),\n",
    "                      chunks(target_tensor_val, BATCH_SIZE))\n",
    "    \n",
    "    for sort_batch, (inp_batch, targ_batch) in enumerate(train_dataset):\n",
    "        lengths = [max(len(x), len(y)) for (x, y) in zip(inp_batch, targ_batch)]\n",
    "        permutation = np.argsort(lengths)\n",
    "        inp_batch = np.array(inp_batch)[permutation]\n",
    "        targ_batch = np.array(targ_batch)[permutation]\n",
    "        \n",
    "        chunked_input = chunks(inp_batch, BATCH_SIZE)\n",
    "        chunked_target = chunks(targ_batch, BATCH_SIZE)\n",
    "        for batch, (inp, targ) in enumerate(zip(chunked_input, chunked_target)):\n",
    "            inp, targ = pad_tensors(inp, targ)\n",
    "            \n",
    "            batch_loss = train_step(encoder, decoder, optimizer, inp, targ)\n",
    "            total_loss += batch_loss\n",
    "            if step % bleu_every_n_steps == 0:\n",
    "                train_bleu(evaluate_bleu_on_batch(inp, targ))\n",
    "                inp, tar = next(val_bleu_iter)\n",
    "                inp, tar = pad_tensors(inp, tar)\n",
    "                test_bleu(evaluate_bleu_on_batch(inp, tar))\n",
    "                test_step(encoder, decoder, inp, tar)\n",
    "            if step % log_metrics_every_n_steps == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f} bleu {:.4f}'.format(\n",
    "                        epoch + 1,\n",
    "                        step,\n",
    "                        batch_loss.numpy(),\n",
    "                        train_bleu.result().numpy()))\n",
    "                log_step = step // log_metrics_every_n_steps\n",
    "                with train_summary_writer.as_default():\n",
    "                    tf.summary.scalar('loss', train_loss.result(), step=log_step)\n",
    "                    tf.summary.scalar('bleu', train_bleu.result(), step=log_step)\n",
    "                with test_summary_writer.as_default():\n",
    "                    tf.summary.scalar('loss', test_loss.result(), step=log_step)\n",
    "                    tf.summary.scalar('bleu', test_bleu.result(), step=log_step)\n",
    "                train_loss.reset_states()\n",
    "                train_bleu.reset_states()\n",
    "                test_loss.reset_states()\n",
    "                test_bleu.reset_states()\n",
    "            step += 1\n",
    "\n",
    "\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (ru, en) in val_dataset.as_numpy_iterator():\n",
    "    x = evaluate_bleu_on_batch(ru, en)\n",
    "    print(x, type(x))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [s.encode('utf-8') for s in ['Довольно странно', 'Хочется выпить чай', 'Когда уже наконец мы выиграем!']]\n",
    "results_by_sentence = translate(texts)\n",
    "\n",
    "for results in results_by_sentence:\n",
    "    for r in results:\n",
    "        if r['result'][-1] != en_tokenizer.word_index['<end>']:\n",
    "            continue\n",
    "        translaton = en_tokenizer.sequences_to_texts([r['result'][1:-1]])\n",
    "        print(translaton)\n",
    "    print('======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.get_memory_usage('GPU:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ref, r['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chencherry = nltk.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.bleu_score.corpus_bleu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[ref] = en_tokenizer.texts_to_sequences([en.decode('utf-8')])\n",
    "nltk.bleu_score.corpus_bleu([[ref]], [r['result'][1:-1]], smoothing_function=chencherry.method7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (ru, en) in datasets['validation'].as_numpy_iterator():\n",
    "    results = translate(ru)\n",
    "    for r in results:\n",
    "        if r['result'][-1] != en_tokenizer.word_index['<end>']:\n",
    "            continue\n",
    "            \n",
    "        print(en_tokenizer.sequences_to_texts([r['result']]))\n",
    "        break\n",
    "    print(en)    \n",
    "    nltk.bleu()\n",
    "    #print(preprocess_sentence(ru))\n",
    "    #print(preprocess_sentence(en))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> <UNK> <end>']\n",
      "['<start> <UNK> decided <end>']\n",
      "['<start> it decided <end>']\n",
      "['<start> he decided <end>']\n",
      "['<start> he decided to take <end>']\n",
      "['<start> he decided to walk <end>']\n",
      "['<start> he decided to go <end>']\n",
      "['<start> he decided to walk in <end>']\n",
      "['<start> he decided to go to <end>']\n",
      "['<start> he decided to go to a <end>']\n",
      "['<start> he decided to go to the <end>']\n",
      "['<start> he decided to take a walk <end>']\n",
      "['<start> he decided to go to the <UNK> <end>']\n",
      "['<start> he decided to go to the walk <end>']\n",
      "['<start> he decided to go to a walk <end>']\n",
      "[\"<start> he decided to go to the <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk ' <end>\"]\n",
      "['<start> he decided to go to a walk in <end>']\n",
      "[\"<start> he decided to go to a walk ' ' <end>\"]\n",
      "['<start> he decided to go to a walk in a <end>']\n",
      "['<start> he decided to go to a walk in the <end>']\n",
      "['<start> he decided to go to a walk in the <UNK> <end>']\n",
      "['<start> he decided to go to a walk in the way <end>']\n",
      "['<start> he decided to go to a walk in a walk <end>']\n",
      "[\"<start> he decided to go to a walk in the <UNK> ' <end>\"]\n",
      "['<start> he decided to go to a walk in a walk ” <end>']\n",
      "[\"<start> he decided to go to a walk in a walk ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he <end>\"]\n",
      "[\"<start> he decided to go to a walk in the <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he went <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he went down <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he went down ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' ' <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <UNK> mt <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ' <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> ” <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> ” <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> ” <UNK> <UNK> <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> ” <UNK> <UNK> <UNK> ” <end>\"]\n",
      "[\"<start> he decided to go to a walk in a walk ' he said <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ” ' <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> ” <UNK> <UNK> ” <UNK> <UNK> <UNK> ” <UNK> <UNK> <UNK> <UNK> <end>\"]\n"
     ]
    }
   ],
   "source": [
    "results = translate('Он решил выйти на прогулку'.encode('utf-8'))\n",
    "for r in results[0]:\n",
    "    if r['result'][-1] != en_tokenizer.word_index['<end>']:\n",
    "        continue\n",
    "    print(en_tokenizer.sequences_to_texts([r['result']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
