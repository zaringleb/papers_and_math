{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Using cached https://files.pythonhosted.org/packages/93/83/85f14bcf27df5ae23502803502f8506eefec18a285fea909aa67dc9b736e/tensorflow_datasets-4.4.0-py3-none-any.whl\n",
      "Collecting dataclasses\n",
      "  Using cached https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Collecting typing_extensions\n",
      "  Using cached https://files.pythonhosted.org/packages/05/e4/baf0031e39cf545f0c9edd5b1a2ea12609b7fcba2d58e118b11753d68cf0/typing_extensions-4.0.1-py3-none-any.whl\n",
      "Collecting importlib_resources\n",
      "  Using cached https://files.pythonhosted.org/packages/24/1b/33e489669a94da3ef4562938cd306e8fa915e13939d7b8277cb5569cb405/importlib_resources-5.4.0-py3-none-any.whl\n",
      "Collecting zipp\n",
      "  Using cached https://files.pythonhosted.org/packages/bd/df/d4a4974a3e3957fd1c1fa3082366d7fff6e428ddb55f074bf64876f8e8ad/zipp-3.6.0-py3-none-any.whl\n",
      "Collecting tensorflow_metadata\n",
      "  Using cached https://files.pythonhosted.org/packages/49/eb/c521f1d457f5e121cb467fa8ad65e7838ad5004d1b0966ebd245da782dfd/tensorflow_metadata-1.2.0-py3-none-any.whl\n",
      "Collecting dill\n",
      "  Using cached https://files.pythonhosted.org/packages/b6/c3/973676ceb86b60835bb3978c6db67a5dc06be6cfdbd14ef0f5a13e3fc9fd/dill-0.3.4-py2.py3-none-any.whl\n",
      "Collecting promise\n",
      "Installing collected packages: tensorflow-datasets, dataclasses, typing-extensions, importlib-resources, zipp, tensorflow-metadata, dill, promise\n",
      "Successfully installed dataclasses-0.8 dill-0.3.4 importlib-resources-5.4.0 promise-2.3 tensorflow-datasets-4.4.0 tensorflow-metadata-1.2.0 typing-extensions-4.0.1 zipp-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow_datasets dataclasses typing_extensions importlib_resources zipp tensorflow_metadata dill promise --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dir = '/home/andysilv/yandexsdc/seminars/attention/datasets'\n",
    "if not os.path.exists(datasets_dir):\n",
    "    os.makedirs(datasets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_TOKENIZER_PATH = 'en_tokenizer.json'\n",
    "RU_TOKENIZER_PATH = 'ru_tokenizer.json'\n",
    "NUM_WORDS = 30000\n",
    "NO_CACHED_TOKENIZER = False\n",
    "MAX_LENGTH = 50 + 2\n",
    "\n",
    "\n",
    "def init_tokenizer(tokenizer_path, texts):\n",
    "    if not os.path.exists(tokenizer_path) or NO_CACHED_TOKENIZER:\n",
    "        print('initializing tokenizer and storing it to', tokenizer_path)\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=NUM_WORDS,\n",
    "            filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
    "            lower=True, split=' ', char_level=False, oov_token='<UNK>',\n",
    "            document_count=0\n",
    "        )\n",
    "\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        with open(tokenizer_path, 'w') as f:\n",
    "            f.write(tokenizer.to_json())\n",
    "    else:\n",
    "        print('loading tokenizer from', tokenizer_path)\n",
    "        with open(tokenizer_path, 'r') as f:\n",
    "            tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_datasets():\n",
    "    config = tfds.translate.wmt.WmtConfig(\n",
    "        version=\"0.0.1\",\n",
    "        language_pair=(\"ru\", \"en\"),\n",
    "        subsets={\n",
    "            tfds.Split.TRAIN: [\"yandexcorpus\"],\n",
    "            tfds.Split.VALIDATION: [\"newstest2012\", 'newstest2013', 'newstest2014', 'newstest2015', 'newstest2016', 'newstest2017'],\n",
    "        },\n",
    "    )\n",
    "    builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "\n",
    "    download_config = tfds.download.DownloadConfig(manual_dir=datasets_dir, extract_dir=datasets_dir)\n",
    "    builder.download_and_prepare(download_config=download_config, download_dir=datasets_dir)\n",
    "    return builder.as_dataset(as_supervised=True)\n",
    "\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    return '<start> ' + s.decode('utf-8') + ' <end>'\n",
    "\n",
    "\n",
    "def build_tokenizers(datasets):\n",
    "    ru_texts, en_texts = zip(*[\n",
    "        (preprocess_sentence(ru), preprocess_sentence(en))\n",
    "        for ru, en in datasets['train'].as_numpy_iterator()])\n",
    "    en_tokenizer = init_tokenizer(EN_TOKENIZER_PATH, en_texts)\n",
    "    ru_tokenizer = init_tokenizer(RU_TOKENIZER_PATH, ru_texts)\n",
    "    \n",
    "    return ru_tokenizer, en_tokenizer, ru_texts, en_texts\n",
    "\n",
    "\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Using custom data configuration ru-en\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer from en_tokenizer.json\n",
      "loading tokenizer from ru_tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "ru_tokenizer, en_tokenizer, ru_texts, en_texts = build_tokenizers(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = ru_tokenizer.texts_to_sequences(ru_texts)\n",
    "target_tensor = en_tokenizer.texts_to_sequences(en_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor = zip(*[(ru, en) for ru, en in zip(input_tensor, target_tensor)\n",
    "                                    if len(ru) <= MAX_LENGTH and len(en) <= MAX_LENGTH])\n",
    "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 52)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen=max_length_inp, padding='post', value=0)\n",
    "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, maxlen=max_length_tar, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764070, 764070, 191018, 191018)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE // BATCH_SIZE\n",
    "WORDS_EMBEDDING_SIZE = 256 # 256\n",
    "HIDDEN_STATES = 1000 # 1024\n",
    "vocab_inp_size = ru_tokenizer.num_words + 1  # pad\n",
    "vocab_tar_size = en_tokenizer.num_words + 1  # pad\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            enc_units, return_sequences=True, \n",
    "            return_state=True, \n",
    "            recurrent_activation='sigmoid', \n",
    "            recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            dec_units, return_sequences=True, \n",
    "            return_state=True, \n",
    "            recurrent_activation='sigmoid', \n",
    "            recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self, batch_sz):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, WORDS_EMBEDDING_SIZE, HIDDEN_STATES, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, WORDS_EMBEDDING_SIZE, HIDDEN_STATES, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # ignore paddings in loss, which have 0 index\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_weights(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.4536\n",
      "Epoch 1 Batch 100 Loss 2.6749\n",
      "Epoch 1 Batch 200 Loss 2.3028\n",
      "Epoch 1 Batch 300 Loss 2.4789\n",
      "Epoch 1 Batch 400 Loss 2.4676\n",
      "Epoch 1 Batch 500 Loss 2.2050\n",
      "Epoch 1 Batch 600 Loss 2.5052\n",
      "Epoch 1 Batch 700 Loss 2.4129\n",
      "Epoch 1 Batch 800 Loss 2.2379\n",
      "Epoch 1 Batch 900 Loss 2.4893\n",
      "Epoch 1 Batch 1000 Loss 2.3665\n",
      "Epoch 1 Batch 1100 Loss 2.3412\n",
      "Epoch 1 Batch 1200 Loss 2.0012\n",
      "Epoch 1 Batch 1300 Loss 2.2255\n",
      "Epoch 1 Batch 1400 Loss 2.5297\n",
      "Epoch 1 Batch 1500 Loss 2.0364\n",
      "Epoch 1 Batch 1600 Loss 2.1950\n",
      "Epoch 1 Batch 1700 Loss 2.1993\n",
      "Epoch 1 Batch 1800 Loss 2.2462\n",
      "Epoch 1 Batch 1900 Loss 2.0792\n",
      "Epoch 1 Batch 2000 Loss 1.8850\n",
      "Epoch 1 Batch 2100 Loss 1.9330\n",
      "Epoch 1 Batch 2200 Loss 2.2040\n",
      "Epoch 1 Batch 2300 Loss 1.9729\n",
      "Epoch 1 Batch 2400 Loss 2.0645\n",
      "Epoch 1 Batch 2500 Loss 2.3787\n",
      "Epoch 1 Batch 2600 Loss 1.8867\n",
      "Epoch 1 Batch 2700 Loss 1.8932\n",
      "Epoch 1 Batch 2800 Loss 2.3298\n",
      "Epoch 1 Batch 2900 Loss 1.8589\n",
      "Epoch 1 Batch 3000 Loss 1.9070\n",
      "Epoch 1 Batch 3100 Loss 1.8551\n",
      "Epoch 1 Batch 3200 Loss 1.8971\n",
      "Epoch 1 Batch 3300 Loss 1.7647\n",
      "Epoch 1 Batch 3400 Loss 2.0364\n",
      "Epoch 1 Batch 3500 Loss 1.8472\n",
      "Epoch 1 Batch 3600 Loss 1.8957\n",
      "Epoch 1 Batch 3700 Loss 2.1011\n",
      "Epoch 1 Batch 3800 Loss 1.8596\n",
      "Epoch 1 Batch 3900 Loss 2.0504\n",
      "Epoch 1 Batch 4000 Loss 1.7378\n",
      "Epoch 1 Batch 4100 Loss 1.8630\n",
      "Epoch 1 Batch 4200 Loss 1.7564\n",
      "Epoch 1 Batch 4300 Loss 1.9347\n",
      "Epoch 1 Batch 4400 Loss 1.6530\n",
      "Epoch 1 Batch 4500 Loss 1.8647\n",
      "Epoch 1 Batch 4600 Loss 1.9282\n",
      "Epoch 1 Batch 4700 Loss 2.0159\n",
      "Epoch 1 Batch 4800 Loss 1.7466\n",
      "Epoch 1 Batch 4900 Loss 1.7916\n",
      "Epoch 1 Batch 5000 Loss 1.5367\n",
      "Epoch 1 Batch 5100 Loss 1.7624\n",
      "Epoch 1 Batch 5200 Loss 1.9446\n",
      "Epoch 1 Batch 5300 Loss 1.8833\n",
      "Epoch 1 Batch 5400 Loss 1.8516\n",
      "Epoch 1 Batch 5500 Loss 1.7224\n",
      "Epoch 1 Batch 5600 Loss 1.7218\n",
      "Epoch 1 Batch 5700 Loss 1.7365\n",
      "Epoch 1 Batch 5800 Loss 1.6651\n",
      "Epoch 1 Batch 5900 Loss 1.8246\n",
      "Epoch 1 Batch 6000 Loss 1.8694\n",
      "Epoch 1 Batch 6100 Loss 1.8077\n",
      "Epoch 1 Batch 6200 Loss 1.6577\n",
      "Epoch 1 Batch 6300 Loss 1.7102\n",
      "Epoch 1 Batch 6400 Loss 2.0088\n",
      "Epoch 1 Batch 6500 Loss 2.0543\n",
      "Epoch 1 Batch 6600 Loss 1.7650\n",
      "Epoch 1 Batch 6700 Loss 1.6746\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "encoder.batch_sz = BATCH_SIZE\n",
    "decoder.batch_sz = BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            # dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            dec_input = tf.expand_dims(targ[:, 0], 1)\n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    text = preprocess_sentence(text)\n",
    "    \n",
    "    encoder.batch_sz = 1\n",
    "    decoder.batch_sz = 1\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    input_tensor = np.array(ru_tokenizer.texts_to_sequences([text]))\n",
    "    enc_output, enc_hidden = encoder(input_tensor, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = np.array([[en_tokenizer.word_index['<start>']]])\n",
    "    \n",
    "    beam_width = 3\n",
    "    max_len = 50\n",
    "    initial_result = {\n",
    "        \"result\": [en_tokenizer.word_index['<start>']],\n",
    "        \"log_prob\": 0\n",
    "    }\n",
    "    \n",
    "    results = [initial_result]\n",
    "    current_states = [dict(\n",
    "        dec_hidden=enc_hidden,\n",
    "        dec_input=dec_input,\n",
    "        **initial_result\n",
    "    )]\n",
    "    \n",
    "    for t in range(1, max_len):\n",
    "        new_current_states = []\n",
    "        for current_state in current_states:\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(\n",
    "                current_state['dec_input'], current_state['dec_hidden'], enc_output)\n",
    "            max_prob_inds = np.argpartition(predictions, -beam_width)[0, -beam_width:]\n",
    "            for ind in max_prob_inds:\n",
    "                probs = np.exp(predictions[0]) / np.sum(np.exp(predictions[0]))\n",
    "                res = {\n",
    "                    'log_prob': current_state['log_prob'] + np.log(probs[ind]),\n",
    "                    'result': current_state['result'] + [ind]\n",
    "                }\n",
    "                results.append(res)\n",
    "                \n",
    "                if ind == en_tokenizer.word_index['<end>']:\n",
    "                    continue\n",
    "                new_current_states.append(dict(\n",
    "                    dec_hidden=dec_hidden,\n",
    "                    dec_input=np.array([[ind]]),\n",
    "                    **res\n",
    "                ))\n",
    "        new_current_states.sort(key=lambda x: x['log_prob'])    \n",
    "        new_current_states = new_current_states[-beam_width:]\n",
    "        current_states = new_current_states\n",
    "    \n",
    "    for r in results:\n",
    "        r['normalized_log_prob'] = r['log_prob'] / len(r['result'])\n",
    "    \n",
    "    results.sort(key=lambda k: -r['normalized_log_prob'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> drink tea <end>']\n",
      "['<start> a free tea <end>']\n",
      "['<start> a free tea cup <end>']\n",
      "['<start> a free tea with <end>']\n",
      "['<start> a free tea cup of <end>']\n",
      "['<start> we can a free tea <end>']\n",
      "['<start> we can a free tea cup <end>']\n",
      "['<start> we can a free tea with <end>']\n",
      "['<start> a free tea cup of tea <end>']\n",
      "['<start> we can a free tea cup of <end>']\n",
      "['<start> a free tea cup of tea party <end>']\n",
      "['<start> we can a free tea with tea party <end>']\n",
      "['<start> we can a free tea cup of tea <end>']\n",
      "['<start> a free tea cup of tea party to <UNK> <end>']\n",
      "['<start> we can a free tea cup of tea party <end>']\n",
      "['<start> a free tea cup of tea party to our tea is a cup <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup to <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of <end>']\n",
      "['<start> a free tea cup of tea party to our tea is a cup of tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea is a cup of tea with <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of our tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of our tea party <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to <UNK> <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to <UNK> tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup to <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to <UNK> <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to <UNK> tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to <UNK> <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to <UNK> tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup to <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party to <UNK> <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party to <UNK> tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to <UNK> <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to <UNK> tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party to our tea with a cup <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party to our tea with a cup of <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup to <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to <UNK> <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to <UNK> tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to <UNK> <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to <UNK> tea <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of <end>']\n",
      "['<start> a free tea cup of tea party to our tea with a cup of tea party to a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea party to our tea with a cup of tea <end>']\n"
     ]
    }
   ],
   "source": [
    "results = translate('Выпить чай'.encode('utf-8'))\n",
    "for r in results:\n",
    "    if r['result'][-1] != en_tokenizer.word_index['<end>']:\n",
    "        continue\n",
    "    print(en_tokenizer.sequences_to_texts([r['result']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNSearchTrainer(object):\n",
    "    EN_TOKENIZER_PATH = 'en_tokenizer.json'\n",
    "    RU_TOKENIZER_PATH = 'ru_tokenizer.json'\n",
    "    NO_CACHED_TOKENIZER = True\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        config = tfds.translate.wmt.WmtConfig(\n",
    "            version=\"0.0.1\",\n",
    "            language_pair=(\"ru\", \"en\"),\n",
    "            subsets={\n",
    "                tfds.Split.TRAIN: [\"yandexcorpus\"],\n",
    "                tfds.Split.VALIDATION: [\"newstest2012\", 'newstest2013', 'newstest2014', 'newstest2015', 'newstest2016', 'newstest2017'],\n",
    "            },\n",
    "        )\n",
    "        builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "        \n",
    "        download_config = tfds.download.DownloadConfig(manual_dir=datasets_dir, extract_dir=datasets_dir)\n",
    "        builder.download_and_prepare(download_config=download_config, download_dir=datasets_dir)\n",
    "        \n",
    "        self._datasets = builder.as_dataset(as_supervised=True)\n",
    "        train_examples = self._datasets['train']\n",
    "        val_examples = self._datasets['validation']\n",
    "        \n",
    "        \n",
    "        ru_texts, en_texts = zip(*[\n",
    "                (self._preprocess_sentence(ru), self._preprocess_sentence(en))\n",
    "                for ru, en in train_examples.as_numpy_iterator()])\n",
    "        \n",
    "        self._en_tokenizer = self._init_tokenizer(self.EN_TOKENIZER_PATH, en_texts)\n",
    "        self._ru_tokenizer = self._init_tokenizer(self.RU_TOKENIZER_PATH, ru_texts)\n",
    "        \n",
    "    def preprocess_sentence(self, s):\n",
    "        return '<start> ' + s.decode('utf-8') + ' <end>'\n",
    "        \n",
    "    def _init_tokenizer(self, tokenizer_path, texts):\n",
    "        if not os.path.exists(tokenizer_path) or self.NO_CACHED_TOKENIZER:\n",
    "            tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "                num_words=RnnSearchModel.NUM_WORDS,\n",
    "                filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
    "                lower=True, split=' ', char_level=False, oov_token='<UNK>',\n",
    "                document_count=0\n",
    "            )\n",
    "\n",
    "            tokenizer.fit_on_texts(texts)\n",
    "            with open(tokenizer_path, 'w') as f:\n",
    "                f.write(tokenizer.to_json())\n",
    "        else:\n",
    "            with open(tokenizer_path, 'r') as f:\n",
    "                tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n",
    "        return tokenizer\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        input_tensor, output_tensor = zip(*[\n",
    "            (self.preprocess_sentence(ru), self.preprocess_sentence(en))\n",
    "            for ru, en in dataset.as_numpy_iterator()])\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        rnn_search = RnnSearchModel()\n",
    "        \n",
    "        dataset = self._datasets['train'].shuffle(buffer_size=100) # comment this line if you don't want to shuffle data\n",
    "        dataset = dataset.batch(batch_size=80)     # batch_size=1 if you want to get only one element per step\n",
    "        for e in range(epoch):\n",
    "            print('Epoch', e)\n",
    "            for num_batch, (ru, en) in enumerate(dataset.as_numpy_iterator()):\n",
    "                tokenized_input_sentences = self._en_tokenizer.texts_to_sequences(\n",
    "                    [self._preprocess_sentence(x) for x in en])\n",
    "                tokenized_output_sentences = self._ru_tokenizer.texts_to_sequences(\n",
    "                    [self._preprocess_sentence(x) for x in ru])\n",
    "                \n",
    "                rnn_search((tokenized_input_sentences, tokenized_output_sentences), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RNNSearchTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = trainer._datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_texts, en_texts = zip(*[\n",
    "                (trainer.preprocess_sentence(ru), trainer.preprocess_sentence(en))\n",
    "                for ru, en in dataset.as_numpy_iterator()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "for ru, en in dataset.as_numpy_iterator():\n",
    "    print(len(trainer._ru_tokenizer.texts_to_sequences([trainer._preprocess_sentence(ru)])[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[74, 3075]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer._en_tokenizer.texts_to_sequences(['Some strange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_lengths, en_lengths = zip(*[(len(trainer._ru_tokenizer.texts_to_sequences([trainer._preprocess_sentence(ru)])[0]), \n",
    "                                len(trainer._en_tokenizer.texts_to_sequences([trainer._preprocess_sentence(en)])[0])) \n",
    "                               for ru, en in dataset.as_numpy_iterator()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(ru_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> The author of the catalogue also draws the attention of the readers to differences in the presentation of the material received, depending on whether they came from an archaeological museum or from an institution incidentally in the possession of such a type of collection. <end>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (ru, en) in trainer._datasets['train'].as_numpy_iterator():\n",
    "    print(trainer._preprocess_sentence(en))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def f(x, y):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "dataset = train_examples.shuffle(buffer_size=100) # comment this line if you don't want to shuffle data\n",
    "dataset = dataset.batch(batch_size=80)     # batch_size=1 if you want to get only one element per step\n",
    "\n",
    "num_batch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (ru, en) in train_examples.as_numpy_iterator():\n",
    "    x = en_tokenizer.texts_to_sequences([en.decode()])\n",
    "    x[0] = [0] * 4 + x[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 1, 1490, 2, 1, 7170, 46, 6862, 1, 655, 2, 1, 3634, 4, 2118, 5, 1, 2051, 2, 1, 610, 615, 1715, 10, 425, 35, 478, 20, 27, 8486, 1234, 19, 20, 27, 2278, 8785, 5, 1, 4713, 2, 59, 6, 355, 2, 1210]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(s) for s in x]\n",
    "max_len = max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 14, 30000)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    x[i] = [0] * (max_len - len(x[i])) + x[i]\n",
    "encoding = tf.one_hot(x, 30000).numpy()\n",
    "for i in range(len(x)):\n",
    "    encoding[i][:max_len - lengths[i]] *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6962"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(i for i, z in enumerate(encoding[0,61]) if z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.one_hot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'and': 3,\n",
       " 'to': 4,\n",
       " 'in': 5,\n",
       " 'a': 6,\n",
       " 'is': 7,\n",
       " 'for': 8,\n",
       " 'that': 9,\n",
       " 'on': 10,\n",
       " 'with': 11,\n",
       " 'as': 12,\n",
       " 'it': 13,\n",
       " 'are': 14,\n",
       " 'be': 15,\n",
       " 'by': 16,\n",
       " 'you': 17,\n",
       " 'this': 18,\n",
       " 'or': 19,\n",
       " 'from': 20,\n",
       " 'was': 21,\n",
       " 'not': 22,\n",
       " 'at': 23,\n",
       " 'have': 24,\n",
       " 'will': 25,\n",
       " 'i': 26,\n",
       " 'an': 27,\n",
       " 'we': 28,\n",
       " 'all': 29,\n",
       " 'which': 30,\n",
       " 'can': 31,\n",
       " 'has': 32,\n",
       " 'but': 33,\n",
       " 'their': 34,\n",
       " 'they': 35,\n",
       " 'he': 36,\n",
       " 'your': 37,\n",
       " 'one': 38,\n",
       " 'if': 39,\n",
       " 'his': 40,\n",
       " 'its': 41,\n",
       " 'more': 42,\n",
       " 'other': 43,\n",
       " 'there': 44,\n",
       " 'were': 45,\n",
       " 'also': 46,\n",
       " 'our': 47,\n",
       " 'when': 48,\n",
       " 'new': 49,\n",
       " 'who': 50,\n",
       " 'been': 51,\n",
       " 'about': 52,\n",
       " 'time': 53,\n",
       " 'only': 54,\n",
       " 'had': 55,\n",
       " 'so': 56,\n",
       " 'these': 57,\n",
       " 'up': 58,\n",
       " 'such': 59,\n",
       " '1': 60,\n",
       " 'any': 61,\n",
       " 'no': 62,\n",
       " 'people': 63,\n",
       " 'into': 64,\n",
       " 'them': 65,\n",
       " 'would': 66,\n",
       " 'may': 67,\n",
       " 'than': 68,\n",
       " 'do': 69,\n",
       " 'out': 70,\n",
       " 'some': 71,\n",
       " 'what': 72,\n",
       " 'first': 73,\n",
       " 'use': 74,\n",
       " '2': 75,\n",
       " 'should': 76,\n",
       " 'most': 77,\n",
       " 'information': 78,\n",
       " 'world': 79,\n",
       " 'after': 80,\n",
       " 'well': 81,\n",
       " 'international': 82,\n",
       " 'my': 83,\n",
       " 'two': 84,\n",
       " 'many': 85,\n",
       " 'work': 86,\n",
       " 'us': 87,\n",
       " 'system': 88,\n",
       " 'russian': 89,\n",
       " 'state': 90,\n",
       " 'then': 91,\n",
       " 'development': 92,\n",
       " '3': 93,\n",
       " 'years': 94,\n",
       " 'over': 95,\n",
       " 'very': 96,\n",
       " 'said': 97,\n",
       " 'like': 98,\n",
       " 'how': 99,\n",
       " 'even': 100,\n",
       " 'now': 101,\n",
       " 'year': 102,\n",
       " 'where': 103,\n",
       " 'between': 104,\n",
       " 'through': 105,\n",
       " 'countries': 106,\n",
       " 'russia': 107,\n",
       " 'those': 108,\n",
       " '5': 109,\n",
       " 'because': 110,\n",
       " 'make': 111,\n",
       " 'could': 112,\n",
       " 'her': 113,\n",
       " 'just': 114,\n",
       " 'during': 115,\n",
       " 'under': 116,\n",
       " 'service': 117,\n",
       " 'services': 118,\n",
       " 'made': 119,\n",
       " 'him': 120,\n",
       " '4': 121,\n",
       " 'national': 122,\n",
       " 'must': 123,\n",
       " 'way': 124,\n",
       " 'each': 125,\n",
       " 'same': 126,\n",
       " 'see': 127,\n",
       " 'used': 128,\n",
       " 'data': 129,\n",
       " 'number': 130,\n",
       " 'government': 131,\n",
       " 'being': 132,\n",
       " 'part': 133,\n",
       " 'business': 134,\n",
       " 'life': 135,\n",
       " 'high': 136,\n",
       " 'including': 137,\n",
       " 'she': 138,\n",
       " 'me': 139,\n",
       " 'company': 140,\n",
       " 'before': 141,\n",
       " 'take': 142,\n",
       " 'while': 143,\n",
       " 'need': 144,\n",
       " 'right': 145,\n",
       " 'day': 146,\n",
       " 'support': 147,\n",
       " 'country': 148,\n",
       " 'states': 149,\n",
       " 'both': 150,\n",
       " 'market': 151,\n",
       " 'economic': 152,\n",
       " 'human': 153,\n",
       " 'without': 154,\n",
       " 'public': 155,\n",
       " 'different': 156,\n",
       " 'however': 157,\n",
       " 'get': 158,\n",
       " 'within': 159,\n",
       " 'shall': 160,\n",
       " 'group': 161,\n",
       " 'own': 162,\n",
       " 'against': 163,\n",
       " 'place': 164,\n",
       " 'good': 165,\n",
       " 'much': 166,\n",
       " 'rights': 167,\n",
       " 'power': 168,\n",
       " 'long': 169,\n",
       " 'free': 170,\n",
       " 'set': 171,\n",
       " '10': 172,\n",
       " 'security': 173,\n",
       " 'order': 174,\n",
       " 'general': 175,\n",
       " 'law': 176,\n",
       " 'important': 177,\n",
       " 'level': 178,\n",
       " 'access': 179,\n",
       " 'three': 180,\n",
       " '6': 181,\n",
       " 'project': 182,\n",
       " 'based': 183,\n",
       " 'following': 184,\n",
       " 'since': 185,\n",
       " 'using': 186,\n",
       " 'know': 187,\n",
       " 'does': 188,\n",
       " 'great': 189,\n",
       " 'every': 190,\n",
       " 'united': 191,\n",
       " 'social': 192,\n",
       " 'another': 193,\n",
       " 'last': 194,\n",
       " 'management': 195,\n",
       " 'process': 196,\n",
       " 'control': 197,\n",
       " 'area': 198,\n",
       " 'help': 199,\n",
       " 's': 200,\n",
       " 'want': 201,\n",
       " 'health': 202,\n",
       " 'available': 203,\n",
       " 'still': 204,\n",
       " 'case': 205,\n",
       " 'example': 206,\n",
       " 'water': 207,\n",
       " 'did': 208,\n",
       " 'e': 209,\n",
       " 'political': 210,\n",
       " '7': 211,\n",
       " 'local': 212,\n",
       " 'end': 213,\n",
       " 'president': 214,\n",
       " 'possible': 215,\n",
       " 'according': 216,\n",
       " 'back': 217,\n",
       " 'financial': 218,\n",
       " 'children': 219,\n",
       " 'energy': 220,\n",
       " 'here': 221,\n",
       " 'room': 222,\n",
       " 'city': 223,\n",
       " 'change': 224,\n",
       " 'server': 225,\n",
       " 'name': 226,\n",
       " 'policy': 227,\n",
       " 'down': 228,\n",
       " 'find': 229,\n",
       " 'go': 230,\n",
       " 'working': 231,\n",
       " \"it's\": 232,\n",
       " 'foreign': 233,\n",
       " '8': 234,\n",
       " 'production': 235,\n",
       " 'provide': 236,\n",
       " 'best': 237,\n",
       " 'among': 238,\n",
       " 'main': 239,\n",
       " 'special': 240,\n",
       " 'european': 241,\n",
       " 'activities': 242,\n",
       " 'become': 243,\n",
       " 'large': 244,\n",
       " 'region': 245,\n",
       " 'come': 246,\n",
       " 'companies': 247,\n",
       " 'per': 248,\n",
       " 'view': 249,\n",
       " 'small': 250,\n",
       " 'million': 251,\n",
       " 'program': 252,\n",
       " 'products': 253,\n",
       " 'god': 254,\n",
       " 'second': 255,\n",
       " 'means': 256,\n",
       " 'think': 257,\n",
       " 'article': 258,\n",
       " 'site': 259,\n",
       " 'around': 260,\n",
       " 'windows': 261,\n",
       " 'say': 262,\n",
       " 'quality': 263,\n",
       " 'open': 264,\n",
       " 'members': 265,\n",
       " 'bank': 266,\n",
       " '000': 267,\n",
       " 'several': 268,\n",
       " 'already': 269,\n",
       " 'next': 270,\n",
       " 'internet': 271,\n",
       " 'global': 272,\n",
       " 'education': 273,\n",
       " 'always': 274,\n",
       " 'report': 275,\n",
       " 'fact': 276,\n",
       " 'called': 277,\n",
       " 'systems': 278,\n",
       " 'man': 279,\n",
       " 'non': 280,\n",
       " 'meeting': 281,\n",
       " 'form': 282,\n",
       " 'moscow': 283,\n",
       " 'problems': 284,\n",
       " 'full': 285,\n",
       " 'today': 286,\n",
       " 'war': 287,\n",
       " 'future': 288,\n",
       " 'period': 289,\n",
       " 'old': 290,\n",
       " 'experience': 291,\n",
       " 'necessary': 292,\n",
       " 'conditions': 293,\n",
       " 'course': 294,\n",
       " 'real': 295,\n",
       " 'women': 296,\n",
       " 'create': 297,\n",
       " 'office': 298,\n",
       " 'off': 299,\n",
       " 'network': 300,\n",
       " 'too': 301,\n",
       " 'party': 302,\n",
       " 'resources': 303,\n",
       " 'europe': 304,\n",
       " 'research': 305,\n",
       " 'republic': 306,\n",
       " '0': 307,\n",
       " '–': 308,\n",
       " 'days': 309,\n",
       " 'found': 310,\n",
       " 'current': 311,\n",
       " 'click': 312,\n",
       " 'often': 313,\n",
       " 'able': 314,\n",
       " 'file': 315,\n",
       " 'few': 316,\n",
       " 'given': 317,\n",
       " 'regional': 318,\n",
       " 'point': 319,\n",
       " 'never': 320,\n",
       " 'web': 321,\n",
       " 'less': 322,\n",
       " 'value': 323,\n",
       " 'central': 324,\n",
       " 'why': 325,\n",
       " 'oil': 326,\n",
       " '20': 327,\n",
       " '9': 328,\n",
       " 'give': 329,\n",
       " 'results': 330,\n",
       " 'application': 331,\n",
       " 'trade': 332,\n",
       " 'further': 333,\n",
       " \"don't\": 334,\n",
       " 'person': 335,\n",
       " 'times': 336,\n",
       " 'include': 337,\n",
       " 'result': 338,\n",
       " 'air': 339,\n",
       " 'software': 340,\n",
       " 'private': 341,\n",
       " 'building': 342,\n",
       " 'member': 343,\n",
       " 'provided': 344,\n",
       " 'home': 345,\n",
       " '15': 346,\n",
       " 'things': 347,\n",
       " 'might': 348,\n",
       " 'areas': 349,\n",
       " 'once': 350,\n",
       " 'key': 351,\n",
       " 'present': 352,\n",
       " 'better': 353,\n",
       " 'problem': 354,\n",
       " 'type': 355,\n",
       " 'family': 356,\n",
       " 'society': 357,\n",
       " 'computer': 358,\n",
       " 'protection': 359,\n",
       " 'account': 360,\n",
       " 'issues': 361,\n",
       " 'body': 362,\n",
       " 'list': 363,\n",
       " 'legal': 364,\n",
       " 'increase': 365,\n",
       " 'upon': 366,\n",
       " 'situation': 367,\n",
       " 'together': 368,\n",
       " 'again': 369,\n",
       " '2008': 370,\n",
       " 'start': 371,\n",
       " '30': 372,\n",
       " 'others': 373,\n",
       " 'organization': 374,\n",
       " 'money': 375,\n",
       " 'committee': 376,\n",
       " 'away': 377,\n",
       " 'center': 378,\n",
       " 'cannot': 379,\n",
       " 'action': 380,\n",
       " '•': 381,\n",
       " 'cooperation': 382,\n",
       " 'making': 383,\n",
       " 'technology': 384,\n",
       " '12': 385,\n",
       " 'little': 386,\n",
       " 'ukraine': 387,\n",
       " 'media': 388,\n",
       " 'training': 389,\n",
       " 'growth': 390,\n",
       " 'industry': 391,\n",
       " 'due': 392,\n",
       " 'far': 393,\n",
       " 'gas': 394,\n",
       " 'going': 395,\n",
       " '2007': 396,\n",
       " 'particular': 397,\n",
       " 'needs': 398,\n",
       " 'various': 399,\n",
       " 'projects': 400,\n",
       " 'term': 401,\n",
       " 'user': 402,\n",
       " 'centre': 403,\n",
       " 'total': 404,\n",
       " 'having': 405,\n",
       " 'line': 406,\n",
       " 'equipment': 407,\n",
       " 'technical': 408,\n",
       " 'put': 409,\n",
       " 'head': 410,\n",
       " 'nations': 411,\n",
       " 'certain': 412,\n",
       " 'organizations': 413,\n",
       " 'council': 414,\n",
       " 'natural': 415,\n",
       " 'major': 416,\n",
       " 'look': 417,\n",
       " 'taken': 418,\n",
       " 'exchange': 419,\n",
       " 'property': 420,\n",
       " 'parties': 421,\n",
       " 'facilities': 422,\n",
       " 'interest': 423,\n",
       " 'school': 424,\n",
       " 'whether': 425,\n",
       " 'military': 426,\n",
       " '2009': 427,\n",
       " 'changes': 428,\n",
       " 'until': 429,\n",
       " 'role': 430,\n",
       " 'above': 431,\n",
       " 'community': 432,\n",
       " 'history': 433,\n",
       " 'something': 434,\n",
       " 'food': 435,\n",
       " 'court': 436,\n",
       " 'whole': 437,\n",
       " 'show': 438,\n",
       " 'known': 439,\n",
       " 'left': 440,\n",
       " 'held': 441,\n",
       " 'terms': 442,\n",
       " 'yet': 443,\n",
       " 'mr': 444,\n",
       " 'u': 445,\n",
       " 'later': 446,\n",
       " 'agreement': 447,\n",
       " 'plan': 448,\n",
       " 'conference': 449,\n",
       " 'environment': 450,\n",
       " 'took': 451,\n",
       " 'least': 452,\n",
       " 'field': 453,\n",
       " 'groups': 454,\n",
       " 'common': 455,\n",
       " 'related': 456,\n",
       " 'house': 457,\n",
       " 'select': 458,\n",
       " 'page': 459,\n",
       " 'hand': 460,\n",
       " 'light': 461,\n",
       " 'product': 462,\n",
       " 'measures': 463,\n",
       " 'four': 464,\n",
       " 'implementation': 465,\n",
       " 'thus': 466,\n",
       " 'b': 467,\n",
       " 'men': 468,\n",
       " 'union': 469,\n",
       " 'price': 470,\n",
       " 'says': 471,\n",
       " 'economy': 472,\n",
       " 'design': 473,\n",
       " 'decision': 474,\n",
       " 'authorities': 475,\n",
       " 'especially': 476,\n",
       " 'young': 477,\n",
       " 'came': 478,\n",
       " 'users': 479,\n",
       " 'third': 480,\n",
       " 'developed': 481,\n",
       " 'therefore': 482,\n",
       " 'additional': 483,\n",
       " 'investment': 484,\n",
       " '11': 485,\n",
       " 'really': 486,\n",
       " 'single': 487,\n",
       " 'ensure': 488,\n",
       " 'specific': 489,\n",
       " 'c': 490,\n",
       " 'higher': 491,\n",
       " 'space': 492,\n",
       " 'performance': 493,\n",
       " 'required': 494,\n",
       " 'basis': 495,\n",
       " 'love': 496,\n",
       " 'american': 497,\n",
       " 'live': 498,\n",
       " 'months': 499,\n",
       " 'percent': 500,\n",
       " 'cost': 501,\n",
       " 'capital': 502,\n",
       " 'knowledge': 503,\n",
       " '2010': 504,\n",
       " 'five': 505,\n",
       " 'language': 506,\n",
       " 'question': 507,\n",
       " 'almost': 508,\n",
       " 'side': 509,\n",
       " 'construction': 510,\n",
       " 'personal': 511,\n",
       " 'am': 512,\n",
       " 'soviet': 513,\n",
       " 'low': 514,\n",
       " 'cases': 515,\n",
       " 'risk': 516,\n",
       " 'activity': 517,\n",
       " 'issue': 518,\n",
       " 'force': 519,\n",
       " 'south': 520,\n",
       " 'code': 521,\n",
       " 'living': 522,\n",
       " 'population': 523,\n",
       " 'board': 524,\n",
       " 'works': 525,\n",
       " 'china': 526,\n",
       " 'rather': 527,\n",
       " 'past': 528,\n",
       " 'located': 529,\n",
       " 'address': 530,\n",
       " 'offer': 531,\n",
       " 'close': 532,\n",
       " 'director': 533,\n",
       " 'tv': 534,\n",
       " 'although': 535,\n",
       " 'nuclear': 536,\n",
       " 'sector': 537,\n",
       " 'though': 538,\n",
       " 'university': 539,\n",
       " 'team': 540,\n",
       " 'provides': 541,\n",
       " 'developing': 542,\n",
       " 'modern': 543,\n",
       " 'land': 544,\n",
       " 'safety': 545,\n",
       " 'effective': 546,\n",
       " 'rate': 547,\n",
       " 'press': 548,\n",
       " 'child': 549,\n",
       " 'range': 550,\n",
       " 'early': 551,\n",
       " 'game': 552,\n",
       " 'created': 553,\n",
       " 'care': 554,\n",
       " 'position': 555,\n",
       " 'sea': 556,\n",
       " 'itself': 557,\n",
       " 'offers': 558,\n",
       " 'm': 559,\n",
       " 'amount': 560,\n",
       " 'taking': 561,\n",
       " 'church': 562,\n",
       " 'top': 563,\n",
       " 'share': 564,\n",
       " 'keep': 565,\n",
       " 'became': 566,\n",
       " 'source': 567,\n",
       " 'car': 568,\n",
       " 'features': 569,\n",
       " 'believe': 570,\n",
       " 'individual': 571,\n",
       " 'run': 572,\n",
       " '100': 573,\n",
       " 'hard': 574,\n",
       " 'operation': 575,\n",
       " 'call': 576,\n",
       " 'lot': 577,\n",
       " 'addition': 578,\n",
       " '2006': 579,\n",
       " 'either': 580,\n",
       " 'convention': 581,\n",
       " 'music': 582,\n",
       " 'date': 583,\n",
       " 'done': 584,\n",
       " 'documents': 585,\n",
       " 'files': 586,\n",
       " 'nature': 587,\n",
       " 'size': 588,\n",
       " 'continue': 589,\n",
       " 'capacity': 590,\n",
       " 'note': 591,\n",
       " 'play': 592,\n",
       " 'etc': 593,\n",
       " 'established': 594,\n",
       " 'clear': 595,\n",
       " 'standard': 596,\n",
       " 'box': 597,\n",
       " 'table': 598,\n",
       " 'programme': 599,\n",
       " 'institutions': 600,\n",
       " 'themselves': 601,\n",
       " '2005': 602,\n",
       " 'event': 603,\n",
       " 'events': 604,\n",
       " 'content': 605,\n",
       " 'started': 606,\n",
       " '50': 607,\n",
       " 'department': 608,\n",
       " 'materials': 609,\n",
       " 'material': 610,\n",
       " 'ministry': 611,\n",
       " 'enough': 612,\n",
       " 'relations': 613,\n",
       " 'self': 614,\n",
       " 'received': 615,\n",
       " 'potential': 616,\n",
       " 'art': 617,\n",
       " 'video': 618,\n",
       " 'staff': 619,\n",
       " 'treatment': 620,\n",
       " 'let': 621,\n",
       " 'thing': 622,\n",
       " 'microsoft': 623,\n",
       " 'read': 624,\n",
       " 'half': 625,\n",
       " 'persons': 626,\n",
       " 'minister': 627,\n",
       " 'words': 628,\n",
       " 'study': 629,\n",
       " 'increased': 630,\n",
       " 'big': 631,\n",
       " 'along': 632,\n",
       " 'short': 633,\n",
       " 'century': 634,\n",
       " 'efforts': 635,\n",
       " 'everything': 636,\n",
       " 'true': 637,\n",
       " 'kind': 638,\n",
       " 'standards': 639,\n",
       " 'search': 640,\n",
       " 'billion': 641,\n",
       " 'requirements': 642,\n",
       " 'makes': 643,\n",
       " '2003': 644,\n",
       " 'book': 645,\n",
       " 'add': 646,\n",
       " 'rules': 647,\n",
       " '25': 648,\n",
       " 'age': 649,\n",
       " 'class': 650,\n",
       " 'got': 651,\n",
       " 'sure': 652,\n",
       " 'active': 653,\n",
       " '14': 654,\n",
       " 'attention': 655,\n",
       " 'document': 656,\n",
       " 'commission': 657,\n",
       " 'model': 658,\n",
       " 'mind': 659,\n",
       " 'contact': 660,\n",
       " 'subject': 661,\n",
       " 'included': 662,\n",
       " 'develop': 663,\n",
       " 'medical': 664,\n",
       " 'check': 665,\n",
       " 'nothing': 666,\n",
       " 'strong': 667,\n",
       " 'went': 668,\n",
       " 'existing': 669,\n",
       " 'applications': 670,\n",
       " 'visit': 671,\n",
       " 'cultural': 672,\n",
       " 'official': 673,\n",
       " '16': 674,\n",
       " 'hours': 675,\n",
       " 'tax': 676,\n",
       " 'considered': 677,\n",
       " 'section': 678,\n",
       " 'levels': 679,\n",
       " 'turn': 680,\n",
       " 'quite': 681,\n",
       " 'try': 682,\n",
       " 'pay': 683,\n",
       " 'screen': 684,\n",
       " 'professional': 685,\n",
       " 'status': 686,\n",
       " 'similar': 687,\n",
       " 'operations': 688,\n",
       " '13': 689,\n",
       " 'forces': 690,\n",
       " 'north': 691,\n",
       " 'scientific': 692,\n",
       " 'understand': 693,\n",
       " 'face': 694,\n",
       " 'reason': 695,\n",
       " 'includes': 696,\n",
       " 'ever': 697,\n",
       " 'death': 698,\n",
       " 'approach': 699,\n",
       " 'prices': 700,\n",
       " 'connection': 701,\n",
       " 'operating': 702,\n",
       " 'feel': 703,\n",
       " 'assistance': 704,\n",
       " 'civil': 705,\n",
       " 'word': 706,\n",
       " 'crisis': 707,\n",
       " 'd': 708,\n",
       " '24': 709,\n",
       " 'told': 710,\n",
       " 'ago': 711,\n",
       " 'programs': 712,\n",
       " 'culture': 713,\n",
       " '18': 714,\n",
       " 'former': 715,\n",
       " 'kazakhstan': 716,\n",
       " 'began': 717,\n",
       " 'analysis': 718,\n",
       " 'g': 719,\n",
       " 'drive': 720,\n",
       " 'please': 721,\n",
       " 'built': 722,\n",
       " 'transport': 723,\n",
       " 'environmental': 724,\n",
       " 'respect': 725,\n",
       " 'text': 726,\n",
       " 'allow': 727,\n",
       " 'students': 728,\n",
       " 'western': 729,\n",
       " 'difficult': 730,\n",
       " 'seen': 731,\n",
       " 'step': 732,\n",
       " 'hotel': 733,\n",
       " 'image': 734,\n",
       " 'recent': 735,\n",
       " 'federation': 736,\n",
       " 'physical': 737,\n",
       " 'appropriate': 738,\n",
       " 'basic': 739,\n",
       " 'east': 740,\n",
       " 'significant': 741,\n",
       " 'act': 742,\n",
       " 'version': 743,\n",
       " 'easy': 744,\n",
       " 'himself': 745,\n",
       " 'near': 746,\n",
       " 'complex': 747,\n",
       " 'complete': 748,\n",
       " 'technologies': 749,\n",
       " 'december': 750,\n",
       " 'effect': 751,\n",
       " 'receive': 752,\n",
       " 'leading': 753,\n",
       " 'choose': 754,\n",
       " 'purpose': 755,\n",
       " '2004': 756,\n",
       " 'administration': 757,\n",
       " 'heart': 758,\n",
       " 'march': 759,\n",
       " 'beginning': 760,\n",
       " 'regions': 761,\n",
       " 'peace': 762,\n",
       " 'costs': 763,\n",
       " 'actions': 764,\n",
       " 'message': 765,\n",
       " 'participants': 766,\n",
       " 'demand': 767,\n",
       " 'september': 768,\n",
       " 'june': 769,\n",
       " \"'\": 770,\n",
       " 'return': 771,\n",
       " 'eu': 772,\n",
       " 'goods': 773,\n",
       " 'science': 774,\n",
       " 'idea': 775,\n",
       " 'reports': 776,\n",
       " 'thought': 777,\n",
       " 'matter': 778,\n",
       " 'v': 779,\n",
       " '2000': 780,\n",
       " 'supply': 781,\n",
       " 'police': 782,\n",
       " 'workers': 783,\n",
       " 'points': 784,\n",
       " 'strategy': 785,\n",
       " 'independent': 786,\n",
       " 'meet': 787,\n",
       " 'across': 788,\n",
       " 'markets': 789,\n",
       " 'questions': 790,\n",
       " 'practice': 791,\n",
       " 'pool': 792,\n",
       " 'phone': 793,\n",
       " 'relevant': 794,\n",
       " 'simply': 795,\n",
       " 'towards': 796,\n",
       " 'outside': 797,\n",
       " 'card': 798,\n",
       " 'bring': 799,\n",
       " 'parts': 800,\n",
       " 'policies': 801,\n",
       " 'customers': 802,\n",
       " 'income': 803,\n",
       " 'industrial': 804,\n",
       " 'needed': 805,\n",
       " 'germany': 806,\n",
       " 'st': 807,\n",
       " 'structure': 808,\n",
       " 'instead': 809,\n",
       " 'ways': 810,\n",
       " 'stage': 811,\n",
       " 'client': 812,\n",
       " 'solution': 813,\n",
       " 'directly': 814,\n",
       " 'cause': 815,\n",
       " 'federal': 816,\n",
       " 'earth': 817,\n",
       " 'october': 818,\n",
       " 'week': 819,\n",
       " 'wide': 820,\n",
       " 'sales': 821,\n",
       " 'night': 822,\n",
       " 'institute': 823,\n",
       " 'device': 824,\n",
       " 'west': 825,\n",
       " 'black': 826,\n",
       " 'direct': 827,\n",
       " 'particularly': 828,\n",
       " 'opportunity': 829,\n",
       " 'white': 830,\n",
       " 'six': 831,\n",
       " 'interests': 832,\n",
       " 'representatives': 833,\n",
       " 'online': 834,\n",
       " 'plans': 835,\n",
       " 'progress': 836,\n",
       " 'move': 837,\n",
       " 'longer': 838,\n",
       " 'simple': 839,\n",
       " 'allows': 840,\n",
       " 'impact': 841,\n",
       " 'request': 842,\n",
       " 'laws': 843,\n",
       " 'leaders': 844,\n",
       " 'budget': 845,\n",
       " 'joint': 846,\n",
       " 'fully': 847,\n",
       " 'asked': 848,\n",
       " 'unique': 849,\n",
       " 'framework': 850,\n",
       " 'limited': 851,\n",
       " 'types': 852,\n",
       " '17': 853,\n",
       " 'average': 854,\n",
       " 'front': 855,\n",
       " 'improve': 856,\n",
       " 'machine': 857,\n",
       " 'freedom': 858,\n",
       " 'january': 859,\n",
       " 'comes': 860,\n",
       " 'minutes': 861,\n",
       " 'led': 862,\n",
       " 'lead': 863,\n",
       " 'km': 864,\n",
       " 'station': 865,\n",
       " 'looking': 866,\n",
       " 'experts': 867,\n",
       " 'traditional': 868,\n",
       " 'belarus': 869,\n",
       " 'internal': 870,\n",
       " 'soon': 871,\n",
       " 'database': 872,\n",
       " 'month': 873,\n",
       " 'principles': 874,\n",
       " 'added': 875,\n",
       " 'usually': 876,\n",
       " 'mail': 877,\n",
       " 'location': 878,\n",
       " 'base': 879,\n",
       " 'consider': 880,\n",
       " 'sometimes': 881,\n",
       " 'mobile': 882,\n",
       " 'whose': 883,\n",
       " 'methods': 884,\n",
       " '40': 885,\n",
       " 'partners': 886,\n",
       " 'test': 887,\n",
       " 'likely': 888,\n",
       " 'values': 889,\n",
       " 'actually': 890,\n",
       " 'written': 891,\n",
       " 'settings': 892,\n",
       " 'accordance': 893,\n",
       " 'designed': 894,\n",
       " 'april': 895,\n",
       " 'doing': 896,\n",
       " 'build': 897,\n",
       " 'co': 898,\n",
       " 'territory': 899,\n",
       " 'options': 900,\n",
       " 'below': 901,\n",
       " 'series': 902,\n",
       " 'forms': 903,\n",
       " 'english': 904,\n",
       " 'brought': 905,\n",
       " 'america': 906,\n",
       " 'participation': 907,\n",
       " 'positive': 908,\n",
       " 'review': 909,\n",
       " 'tell': 910,\n",
       " 'session': 911,\n",
       " 'providing': 912,\n",
       " 'red': 913,\n",
       " 'middle': 914,\n",
       " 'anti': 915,\n",
       " 'airport': 916,\n",
       " 'method': 917,\n",
       " 'memory': 918,\n",
       " 'resolution': 919,\n",
       " 'domestic': 920,\n",
       " 'enter': 921,\n",
       " 'agency': 922,\n",
       " 'credit': 923,\n",
       " 'plant': 924,\n",
       " 'town': 925,\n",
       " 'sense': 926,\n",
       " 'ukrainian': 927,\n",
       " 'tools': 928,\n",
       " 'website': 929,\n",
       " 'involved': 930,\n",
       " 'lower': 931,\n",
       " 'road': 932,\n",
       " 'stay': 933,\n",
       " 'asia': 934,\n",
       " 'commercial': 935,\n",
       " 'mode': 936,\n",
       " 'expected': 937,\n",
       " 'processes': 938,\n",
       " 'conflict': 939,\n",
       " 'religious': 940,\n",
       " 'previous': 941,\n",
       " 'moment': 942,\n",
       " 'fund': 943,\n",
       " 'bar': 944,\n",
       " 'success': 945,\n",
       " 'rule': 946,\n",
       " 'post': 947,\n",
       " 'currently': 948,\n",
       " 'stop': 949,\n",
       " 'final': 950,\n",
       " 'shows': 951,\n",
       " 'follow': 952,\n",
       " 'ready': 953,\n",
       " 'carried': 954,\n",
       " 'communication': 955,\n",
       " 'growing': 956,\n",
       " \"that's\": 957,\n",
       " 'responsible': 958,\n",
       " '”': 959,\n",
       " 'monitoring': 960,\n",
       " 'planning': 961,\n",
       " 'greater': 962,\n",
       " 'function': 963,\n",
       " 'citizens': 964,\n",
       " 'german': 965,\n",
       " 'ask': 966,\n",
       " 'task': 967,\n",
       " 'serious': 968,\n",
       " 'sent': 969,\n",
       " 'tool': 970,\n",
       " 'display': 971,\n",
       " 'officials': 972,\n",
       " 'july': 973,\n",
       " 'news': 974,\n",
       " 'executive': 975,\n",
       " 'movement': 976,\n",
       " 'hope': 977,\n",
       " 'mass': 978,\n",
       " 'com': 979,\n",
       " 'net': 980,\n",
       " 'object': 981,\n",
       " 'funds': 982,\n",
       " 'assembly': 983,\n",
       " 'entire': 984,\n",
       " 'option': 985,\n",
       " 'player': 986,\n",
       " 'friends': 987,\n",
       " 'floor': 988,\n",
       " 'november': 989,\n",
       " 'anything': 990,\n",
       " 'lord': 991,\n",
       " 'successful': 992,\n",
       " 'steps': 993,\n",
       " 'p': 994,\n",
       " 'bodies': 995,\n",
       " 'lost': 996,\n",
       " 'transfer': 997,\n",
       " \"i'm\": 998,\n",
       " 'throughout': 999,\n",
       " 'radio': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 62, 30000)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([62, 80, 30000])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(encoding, [1, 0, 2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnSearchModel(tf.keras.Model):\n",
    "    NUM_WORDS = 30000\n",
    "    HIDDEN_STATES = 1000\n",
    "    WORDS_EMBEDDING_SIZE = 620\n",
    "    MAXOUT_HIDDEN_LAYER_SIZE = 500\n",
    "    ALIGNMENT_MODEL_HIDDEN_UNITS = 1000\n",
    "    MAX_OUTPUT_SENTENCE_LEN = 50\n",
    "    \n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RnnSearchModel, self).__init__(**kwargs)\n",
    "        self._forward_enc_weights = self._init_encoder_weights()\n",
    "        self._backward_enc_weights = self._init_encoder_weights()\n",
    "        self._encoder_embedding_layer = tf.keras.layers.Embedding(self.NUM_WORDS, self.WORDS_EMBEDDING_SIZE)\n",
    "        self._decoder_embedding_layer = tf.keras.layers.Embedding(self.NUM_WORDS, self.WORDS_EMBEDDING_SIZE)\n",
    "        self._dec_weights = self._init_decoder_weights()\n",
    "        \n",
    "        \n",
    "    def _init_encoder_weights():\n",
    "        ortho_initializer = tf.keras.initializers.Orthogonal()\n",
    "        normal_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.01)\n",
    "        \n",
    "        use_bias = True\n",
    "        return {\n",
    "            'W': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=normal_initializer),\n",
    "            'Wr': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=normal_initializer),\n",
    "            'Wz': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=normal_initializer),\n",
    "            'U':  tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "            'Ur': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "            'Uz': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "        }\n",
    "    \n",
    "    def _init_decoder_weights(self):\n",
    "        ortho_initializer = tf.keras.initializers.Orthogonal()\n",
    "        normal_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.01)\n",
    "        normal_initializer_001 = tf.keras.initializers.RandomNormal(mean=0., stddev=0.001)\n",
    "        use_bias = True\n",
    "        return {\n",
    "            'va': tf.keras.layers.Dense(units=1, use_bias=use_bias, kernel_initializer=tf.keras.initializers.Zeros()),\n",
    "            'Wa': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=normal_initializer_001),\n",
    "            'Ua': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=normal_initializer_001),\n",
    "            'Ws': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=normal_initializer),\n",
    "            'W': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=normal_initializer),\n",
    "            'Wr': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=normal_initializer),\n",
    "            'Wz': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=normal_initializer),\n",
    "            'U':  tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "            'Ur': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "            'Uz': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "            'C': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "            'Cz': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "            'Cr': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "            'U0': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "            'V0': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "            'C0': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "            'W0': tf.keras.layers.Dense(units=self.HIDDEN_STATES, use_bias=use_bias, kernel_initializer=ortho_initializer),\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def _pad_and_one_hot_encode(self, x):\n",
    "        batch_size = len(x)\n",
    "        lengths = [len(s) for s in x]\n",
    "        max_len = max(lengths)\n",
    "        # pad all input sequences with leading zeros to make their lengths equal to max_len\n",
    "        for i in range(batch_size):\n",
    "            x[i] = x[i] + [0] * (max_len - lengths[i])\n",
    "\n",
    "        x = tf.one_hot(x, self.NUM_WORDS)\n",
    "        return x\n",
    "    \n",
    "    def _get_encoder_states(self, x, weights):\n",
    "        prev_h = tf.zeros(self.HIDDEN_STATES)\n",
    "        hidden_states = []\n",
    "        for x_i in x:\n",
    "            ex_i = self._encoder_embedding_layer(x_i)\n",
    "            r = tf.sigmoid(weights['Wr'](ex_i) + weights['Ur'](prev_h))\n",
    "            z = tf.sigmoid(weights['Wz'](ex_i) + weights['Uz'](prev_h))\n",
    "            cur_h_tmp = tf.keras.activations.tanh(weights['W'](ex_i) + weights['U'](r * prev_h))\n",
    "            cur_h = (1 - z) * prev_h + z * cur_h_tmp\n",
    "            hidden_states.append(cur_h)\n",
    "            prev_h = cur_h\n",
    "        return hidden_states\n",
    "    \n",
    "    def _get_predictions(self, weights, h):\n",
    "        prev_s = tf.tanh(self._dec_weights['Ws'](h[0, self.HIDDEN_STATES:]))\n",
    "        probs = []\n",
    "        for i in range(self.MAX_OUTPUT_SENTENCE_LEN):\n",
    "            e_i = self._dec_weights['va'](tf.tanh(self._dec_weights['Wa'](prev_s) + self._dec_weights['Ua'](h)))\n",
    "            alpha_i = tf.keras.layers.Softmax()(e_i)\n",
    "            c = tf.tensordot(alpha_i, h, 1)\n",
    "            ey_i = self._decoder_embedding_layer(y_i)\n",
    "            r = tf.sigmoid(weights['Wr'](ey_i) + weights['Ur'](prev_s) + weights['Cr'](c))\n",
    "            z = tf.sigmoid(weights['Wz'](ey_i) + weights['Uz'](prev_s) + weights['Cz'](c))\n",
    "            cur_s_tmp = tf.keras.activations.tanh(weights['W'](ey_i) + weights['U'](r * prev_h) + weights['Cr'](c))\n",
    "            cur_s = (1 - z) * prev_s + z * cur_s_tmp\n",
    "            t = weights['U0'](prev_s) + weights['V0'](ey_i) + weights['C0'](c)\n",
    "            t = tfa.layers.Maxout(num_units=self.MAXOUT_HIDDEN_LAYER_SIZE)(t)\n",
    "            probs_i = weights['W0'](t)\n",
    "            prev_s = cur_s\n",
    "            probs.append(probs[i])\n",
    "        return tf.stack(probs)\n",
    "\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self._pad_and_one_hot_encode(inputs)\n",
    "        \n",
    "        # reshape x to be    max_len x VocabSize x batch_size\n",
    "        x = tf.transpose(encoding, [1, 0, 2])\n",
    "        h_forward = self._get_encoder_states(x, self._forward_enc_weights)\n",
    "        h_backward = self._get_encoder_states(x, self._backward_enc_weights)\n",
    "        h = tf.concat([h_forward, h_backward], axis=1, name='h')  # shape must become max_len x batch_size x 2 * HIDDEN_STATES\n",
    "        y_predicted = self._get_predictions(self._dec_weights, h)\n",
    "        return y_predicted\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
