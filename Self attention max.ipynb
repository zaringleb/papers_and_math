{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(qk, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, analytical_solution=False):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        if analytical_solution:\n",
    "            key_and_query_initializer = tf.keras.initializers.Identity(1000)\n",
    "            value_initializer = tf.keras.initializers.Identity()\n",
    "            self.wq = tf.keras.layers.Dense(d_model, use_bias=False, name='Q', kernel_initializer=key_and_query_initializer)\n",
    "            self.wk = tf.keras.layers.Dense(d_model, use_bias=False, name='K', kernel_initializer=key_and_query_initializer)\n",
    "            self.wv = tf.keras.layers.Dense(d_model, use_bias=False, name='V', kernel_initializer=value_initializer)\n",
    "            \n",
    "        else:\n",
    "            self.wq = tf.keras.layers.Dense(d_model, use_bias=False, name='Q')\n",
    "            self.wk = tf.keras.layers.Dense(d_model, use_bias=False, name='K')\n",
    "            self.wv = tf.keras.layers.Dense(d_model, use_bias=False, name='V')\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        q = self.wq(x)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(x)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v)\n",
    "\n",
    "        y = tf.math.reduce_sum(scaled_attention[:, 0], axis=-1)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    \n",
    "model = AttentionModel(2)\n",
    "analytical = AttentionModel(2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'attention_model_1/Q/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[1000.,    0.]], dtype=float32)>,\n",
       " <tf.Variable 'attention_model_1/K/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[1000.,    0.]], dtype=float32)>,\n",
       " <tf.Variable 'attention_model_1/V/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[1., 0.]], dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_input = tf.random.uniform((2, 1), dtype=tf.float32, minval=0, maxval=1)\n",
    "analytical(temp_input)\n",
    "analytical.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(size, batch_size=32):\n",
    "    n = 2\n",
    "    x = np.random.random((size, n, 1))\n",
    "    y = x.max(axis=1)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "train_ds = gen_dataset(500)\n",
    "test_ds = gen_dataset(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.5611439 ]\n",
      " [0.38295653]], shape=(2, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for one, _ in test_ds:\n",
    "    print(one[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer attention_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "tf.Tensor(0.15397447, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for one, _ in test_ds:\n",
    "    print(model(one)[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = tf.keras.metrics.MeanSquaredError(\n",
    "    name='mean_squared_error', dtype=None\n",
    ")\n",
    "\n",
    "test_metric = tf.keras.metrics.MeanSquaredError(\n",
    "    name='test_mean_squared_error', dtype=None\n",
    ")\n",
    "\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer attention_model_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_metric.reset_states()\n",
    "\n",
    "for x, y in test_ds:\n",
    "    predictions = analytical(x, training=False)\n",
    "\n",
    "    test_metric(y, predictions)\n",
    "print(test_metric.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x, training=True)\n",
    "        loss = loss_object(y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    metric(y, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    predictions = model(x, training=False)\n",
    "\n",
    "    test_metric(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train: 0.07857292145490646, Test: 0.29175296425819397, \n",
      "Epoch 2, Train: 0.021382847800850868, Test: 0.031977590173482895, \n",
      "Epoch 3, Train: 0.012054416351020336, Test: 0.01664106361567974, \n",
      "Epoch 4, Train: 0.00808650627732277, Test: 0.00987904891371727, \n",
      "Epoch 5, Train: 0.007203350774943829, Test: 0.009564390406012535, \n",
      "Epoch 6, Train: 0.006553126499056816, Test: 0.00849725678563118, \n",
      "Epoch 7, Train: 0.005928630009293556, Test: 0.007973495870828629, \n",
      "Epoch 8, Train: 0.005238991696387529, Test: 0.0072736581787467, \n",
      "Epoch 9, Train: 0.0045684389770030975, Test: 0.0064782011322677135, \n",
      "Epoch 10, Train: 0.003959957975894213, Test: 0.005720713175833225, \n",
      "Epoch 11, Train: 0.003448086092248559, Test: 0.005061580333858728, \n",
      "Epoch 12, Train: 0.003036940237507224, Test: 0.004518517293035984, \n",
      "Epoch 13, Train: 0.002710059517994523, Test: 0.00407995143905282, \n",
      "Epoch 14, Train: 0.002448303857818246, Test: 0.0037263277918100357, \n",
      "Epoch 15, Train: 0.002235839143395424, Test: 0.003439289052039385, \n",
      "Epoch 16, Train: 0.002060564234852791, Test: 0.003203553846105933, \n",
      "Epoch 17, Train: 0.0019134428584948182, Test: 0.0030066315084695816, \n",
      "Epoch 18, Train: 0.0017877797363325953, Test: 0.002838257933035493, \n",
      "Epoch 19, Train: 0.0016786236083135009, Test: 0.0026899955701082945, \n",
      "Epoch 20, Train: 0.0015823342837393284, Test: 0.002555145649239421, \n",
      "Epoch 21, Train: 0.0014962706482037902, Test: 0.002428935607895255, \n",
      "Epoch 22, Train: 0.0014185826294124126, Test: 0.0023087295703589916, \n",
      "Epoch 23, Train: 0.0013480311026796699, Test: 0.0021940316073596478, \n",
      "Epoch 24, Train: 0.0012838160619139671, Test: 0.00208604265935719, \n",
      "Epoch 25, Train: 0.0012254153843969107, Test: 0.001986746210604906, \n",
      "Epoch 26, Train: 0.0011724357027560472, Test: 0.0018978321459144354, \n",
      "Epoch 27, Train: 0.0011245155474171042, Test: 0.0018198871985077858, \n",
      "Epoch 28, Train: 0.0010812714463099837, Test: 0.0017521774861961603, \n",
      "Epoch 29, Train: 0.0010422897757962346, Test: 0.0016930407145991921, \n",
      "Epoch 30, Train: 0.001007135841064155, Test: 0.0016405413625761867, \n",
      "Epoch 31, Train: 0.0009753719205036759, Test: 0.0015930176014080644, \n",
      "Epoch 32, Train: 0.0009465725743211806, Test: 0.001549296430312097, \n",
      "Epoch 33, Train: 0.0009203467052429914, Test: 0.0015086455969139934, \n",
      "Epoch 34, Train: 0.0008963525178842247, Test: 0.0014706063084304333, \n",
      "Epoch 35, Train: 0.0008743056678213179, Test: 0.0014348631957545877, \n",
      "Epoch 36, Train: 0.0008539691916666925, Test: 0.0014011720195412636, \n",
      "Epoch 37, Train: 0.0008351473370566964, Test: 0.0013693268410861492, \n",
      "Epoch 38, Train: 0.0008176728151738644, Test: 0.0013391587417572737, \n",
      "Epoch 39, Train: 0.0008014006889425218, Test: 0.0013105265097692609, \n",
      "Epoch 40, Train: 0.0007862068596296012, Test: 0.0012833039509132504, \n",
      "Epoch 41, Train: 0.0007719817804172635, Test: 0.001257382333278656, \n",
      "Epoch 42, Train: 0.000758631038479507, Test: 0.0012326602591201663, \n",
      "Epoch 43, Train: 0.000746070290915668, Test: 0.0012090493692085147, \n",
      "Epoch 44, Train: 0.0007342261960729957, Test: 0.001186469686217606, \n",
      "Epoch 45, Train: 0.0007230335613712668, Test: 0.0011648464715108275, \n",
      "Epoch 46, Train: 0.0007124349940568209, Test: 0.001144114532507956, \n",
      "Epoch 47, Train: 0.0007023790385574102, Test: 0.0011242127511650324, \n",
      "Epoch 48, Train: 0.0006928200600668788, Test: 0.0011050842003896832, \n",
      "Epoch 49, Train: 0.0006837166729383171, Test: 0.001086680218577385, \n",
      "Epoch 50, Train: 0.0006750321481376886, Test: 0.0010689528426155448, \n",
      "Epoch 51, Train: 0.0006667339475825429, Test: 0.001051858183927834, \n",
      "Epoch 52, Train: 0.0006587916286662221, Test: 0.0010353588731959462, \n",
      "Epoch 53, Train: 0.0006511793471872807, Test: 0.0010194185888394713, \n",
      "Epoch 54, Train: 0.0006438716081902385, Test: 0.001004001940600574, \n",
      "Epoch 55, Train: 0.0006368474569171667, Test: 0.0009890818037092686, \n",
      "Epoch 56, Train: 0.000630086287856102, Test: 0.000974626571405679, \n",
      "Epoch 57, Train: 0.0006235702894628048, Test: 0.0009606129606254399, \n",
      "Epoch 58, Train: 0.000617282057646662, Test: 0.0009470130898989737, \n",
      "Epoch 59, Train: 0.0006112075643613935, Test: 0.00093380908947438, \n",
      "Epoch 60, Train: 0.0006053328397683799, Test: 0.0009209763375110924, \n",
      "Epoch 61, Train: 0.0005996446707285941, Test: 0.0009084990015253425, \n",
      "Epoch 62, Train: 0.0005941317649558187, Test: 0.0008963578147813678, \n",
      "Epoch 63, Train: 0.0005887833540327847, Test: 0.0008845357806421816, \n",
      "Epoch 64, Train: 0.0005835904157720506, Test: 0.0008730179397389293, \n",
      "Epoch 65, Train: 0.0005785436369478703, Test: 0.0008617914863862097, \n",
      "Epoch 66, Train: 0.000573634693864733, Test: 0.0008508418686687946, \n",
      "Epoch 67, Train: 0.0005688566016033292, Test: 0.0008401572704315186, \n",
      "Epoch 68, Train: 0.0005642014439217746, Test: 0.0008297257008962333, \n",
      "Epoch 69, Train: 0.0005596642149612308, Test: 0.000819539069198072, \n",
      "Epoch 70, Train: 0.0005552382790483534, Test: 0.0008095852099359035, \n",
      "Epoch 71, Train: 0.0005509191541932523, Test: 0.0007998567307367921, \n",
      "Epoch 72, Train: 0.0005467013106681406, Test: 0.0007903443183749914, \n",
      "Epoch 73, Train: 0.000542580964975059, Test: 0.0007810418610461056, \n",
      "Epoch 74, Train: 0.000538553693331778, Test: 0.0007719412678852677, \n",
      "Epoch 75, Train: 0.0005346161196939647, Test: 0.0007630371837876737, \n",
      "Epoch 76, Train: 0.0005307654500938952, Test: 0.0007543233805336058, \n",
      "Epoch 77, Train: 0.0005269983666948974, Test: 0.0007457950268872082, \n",
      "Epoch 78, Train: 0.0005233125993981957, Test: 0.0007374474662356079, \n",
      "Epoch 79, Train: 0.0005197059945203364, Test: 0.0007292777881957591, \n",
      "Epoch 80, Train: 0.0005161762237548828, Test: 0.000721281860023737, \n",
      "Epoch 81, Train: 0.0005127228796482086, Test: 0.0007134575280360878, \n",
      "Epoch 82, Train: 0.0005093445652164519, Test: 0.0007058043265715241, \n",
      "Epoch 83, Train: 0.0005060391267761588, Test: 0.0006983171915635467, \n",
      "Epoch 84, Train: 0.000502808194141835, Test: 0.0006910005467943847, \n",
      "Epoch 85, Train: 0.0004996497882530093, Test: 0.0006838521221652627, \n",
      "Epoch 86, Train: 0.0004965653060935438, Test: 0.0006768735474906862, \n",
      "Epoch 87, Train: 0.0004935551551170647, Test: 0.0006700680241920054, \n",
      "Epoch 88, Train: 0.0004906198009848595, Test: 0.0006634375313296914, \n",
      "Epoch 89, Train: 0.0004877608153037727, Test: 0.0006569872493855655, \n",
      "Epoch 90, Train: 0.00048498043906874955, Test: 0.0006507239304482937, \n",
      "Epoch 91, Train: 0.0004822799819521606, Test: 0.0006446530460380018, \n",
      "Epoch 92, Train: 0.0004796617431566119, Test: 0.0006387831526808441, \n",
      "Epoch 93, Train: 0.0004771295061800629, Test: 0.0006331280455924571, \n",
      "Epoch 94, Train: 0.00047468749107792974, Test: 0.0006276980275288224, \n",
      "Epoch 95, Train: 0.0004723372985608876, Test: 0.0006225082324817777, \n",
      "Epoch 96, Train: 0.0004700849240180105, Test: 0.0006175749585963786, \n",
      "Epoch 97, Train: 0.00046793505316600204, Test: 0.0006129200919531286, \n",
      "Epoch 98, Train: 0.0004658919933717698, Test: 0.0006085635395720601, \n",
      "Epoch 99, Train: 0.0004639616818167269, Test: 0.0006045330665074289, \n",
      "Epoch 100, Train: 0.0004621496482286602, Test: 0.0006008555646985769, \n",
      "Epoch 101, Train: 0.00046046197530813515, Test: 0.0005975647363811731, \n",
      "Epoch 102, Train: 0.00045890529872849584, Test: 0.00059469387633726, \n",
      "Epoch 103, Train: 0.0004574837221298367, Test: 0.0005922786658629775, \n",
      "Epoch 104, Train: 0.00045620257151313126, Test: 0.0005903571145609021, \n",
      "Epoch 105, Train: 0.0004550672019831836, Test: 0.0005889706662856042, \n",
      "Epoch 106, Train: 0.00045408084406517446, Test: 0.0005881591932848096, \n",
      "Epoch 107, Train: 0.00045324512757360935, Test: 0.0005879580858163536, \n",
      "Epoch 108, Train: 0.0004525604599621147, Test: 0.0005884040729142725, \n",
      "Epoch 109, Train: 0.0004520233196672052, Test: 0.0005895228241570294, \n",
      "Epoch 110, Train: 0.0004516300105024129, Test: 0.0005913336062803864, \n",
      "Epoch 111, Train: 0.0004513711028266698, Test: 0.0005938489339314401, \n",
      "Epoch 112, Train: 0.0004512346931733191, Test: 0.0005970596102997661, \n",
      "Epoch 113, Train: 0.00045120500726625323, Test: 0.0006009443895891309, \n",
      "Epoch 114, Train: 0.000451261323178187, Test: 0.0006054617697373033, \n",
      "Epoch 115, Train: 0.0004513795720413327, Test: 0.000610551331192255, \n",
      "Epoch 116, Train: 0.00045153385144658387, Test: 0.0006161381024867296, \n",
      "Epoch 117, Train: 0.0004516952612902969, Test: 0.000622127263341099, \n",
      "Epoch 118, Train: 0.0004518327477853745, Test: 0.0006284057162702084, \n",
      "Epoch 119, Train: 0.0004519133653957397, Test: 0.0006348562892526388, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Train: 0.0004519088543020189, Test: 0.0006413486553356051, \n",
      "Epoch 121, Train: 0.00045179034350439906, Test: 0.0006477597635239363, \n",
      "Epoch 122, Train: 0.00045153105747886, Test: 0.0006539647001773119, \n",
      "Epoch 123, Train: 0.00045110820792615414, Test: 0.000659849145449698, \n",
      "Epoch 124, Train: 0.0004505033139139414, Test: 0.0006653083255514503, \n",
      "Epoch 125, Train: 0.0004497032205108553, Test: 0.000670257315505296, \n",
      "Epoch 126, Train: 0.00044869829434901476, Test: 0.0006746308063156903, \n",
      "Epoch 127, Train: 0.00044748224900104105, Test: 0.0006783681455999613, \n",
      "Epoch 128, Train: 0.00044605377479456365, Test: 0.0006814313237555325, \n",
      "Epoch 129, Train: 0.00044441400677897036, Test: 0.0006838029948994517, \n",
      "Epoch 130, Train: 0.0004425672523211688, Test: 0.0006854658713564277, \n",
      "Epoch 131, Train: 0.00044052008888684213, Test: 0.0006864290335215628, \n",
      "Epoch 132, Train: 0.00043827746412716806, Test: 0.0006866953335702419, \n",
      "Epoch 133, Train: 0.0004358459555078298, Test: 0.000686274841427803, \n",
      "Epoch 134, Train: 0.0004332352546043694, Test: 0.0006851817015558481, \n",
      "Epoch 135, Train: 0.00043045045458711684, Test: 0.0006834393716417253, \n",
      "Epoch 136, Train: 0.00042749804561026394, Test: 0.0006810528575442731, \n",
      "Epoch 137, Train: 0.0004243858275003731, Test: 0.0006780497496947646, \n",
      "Epoch 138, Train: 0.0004211191553622484, Test: 0.0006744484999217093, \n",
      "Epoch 139, Train: 0.0004177009977865964, Test: 0.0006702499813400209, \n",
      "Epoch 140, Train: 0.0004141354584135115, Test: 0.0006654700846411288, \n",
      "Epoch 141, Train: 0.0004104288527742028, Test: 0.0006601227796636522, \n",
      "Epoch 142, Train: 0.0004065810644533485, Test: 0.0006542148184962571, \n",
      "Epoch 143, Train: 0.0004025951202493161, Test: 0.0006477500428445637, \n",
      "Epoch 144, Train: 0.0003984707873314619, Test: 0.0006407289765775204, \n",
      "Epoch 145, Train: 0.0003942126058973372, Test: 0.0006331625627353787, \n",
      "Epoch 146, Train: 0.0003898203431162983, Test: 0.0006250532460398972, \n",
      "Epoch 147, Train: 0.0003852968802675605, Test: 0.0006164074875414371, \n",
      "Epoch 148, Train: 0.0003806448366958648, Test: 0.0006072378600947559, \n",
      "Epoch 149, Train: 0.0003758681414183229, Test: 0.0005975460517220199, \n",
      "Epoch 150, Train: 0.0003709715383592993, Test: 0.0005873657646588981, \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 150\n",
    "history = defaultdict(list)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    metric.reset_states()\n",
    "    test_metric.reset_states()\n",
    "\n",
    "\n",
    "    for x, y in test_ds:\n",
    "        test_step(x, y)\n",
    "        \n",
    "    for x, y in train_ds:\n",
    "        train_step(x, y)\n",
    "        \n",
    "    history['train'].append(metric.result())\n",
    "    history['test'].append(test_metric.result())\n",
    "    \n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Train: {metric.result()}, '\n",
    "        f'Test: {test_metric.result()}, '\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efd900adba8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efdc0319128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['train'], '-', label='train')\n",
    "plt.plot(history['test'], '-', label='test')\n",
    "plt.ylim([0, 0.01])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'attention_model/Q/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[10.98553, -6.47468]], dtype=float32)>,\n",
       " <tf.Variable 'attention_model/K/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[ 14.428955, -10.657505]], dtype=float32)>,\n",
       " <tf.Variable 'attention_model/V/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[ 1.0407985 , -0.06015549]], dtype=float32)>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
